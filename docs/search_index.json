[["index.html", "Chapter 1 Introduction", " AFLEARN Introduction to R using ICAN-ICAR 2025 Survey Data Siphiwe M. Bogatsu February 2026 Chapter 1 Introduction "],["survey-design-sampling-and-weighting.html", "Chapter 2 Survey Design, Sampling, and Weighting 2.1 Survey sampling and design 2.2 Survey weights and design-based inference 2.3 Practice exercises", " Chapter 2 Survey Design, Sampling, and Weighting Household surveys such as PAL Network’s ICAN-ICAR study estimate population learning outcomes from a sample. The sampling design determines which households enter the data, and the survey design variables (weights, strata, and clusters) determine how analysts should estimate population quantities and uncertainty. After this chapter, you should be able to explain probability sampling, describe the ICAN-ICAR multi-stage design, interpret a survey weight, and compute design-correct estimates in R. #| echo: true #| warning: false #| message: false library(tidyverse) library(survey) library(knitr) # load the data dat = read_csv(&quot;data/2025_PAL_ICAN-ICAR_data.csv&quot;, show_col_types = FALSE) ## Warning: One or more parsing issues, call `problems()` on your data frame for details, e.g.: ## dat &lt;- vroom(...) ## problems(dat) 2.1 Survey sampling and design A census measures every household in the population. ICAN-ICAR survey data instead uses a probability sample because a census is rarely feasible at national scale. Probability sampling supports population inference because the design assigns each unit a known (or estimable) selection probability. ICAN-ICAR survey data uses a stratified, multi-stage cluster design.@fig-pal-sampling-process illustrates the logic. Figure 2.1: Illustration of the ICAN-ICAR multi-stage sampling process. Stage 1 (select clusters). The survey defines primary sampling units (PSUs) such as enumeration areas, villages, or urban blocks. Within strata (for example, region-by-urban/rural categories), the design selects PSUs using probability proportional to size (PPS) so that larger PSUs have a higher chance of selection. Stage 2 (select households). Within each selected PSU, the field team selects a fixed number of households using a standardized selection procedure (for example, systematic selection from a listing). This second stage produces the household sample used for data collection. This design shapes analysis in two ways. First, learners within the same PSU tend to resemble one another, which increases uncertainty relative to a simple random sample of the same size. Second, stratification often improves precision because the sample covers key sub-populations more evenly. 2.2 Survey weights and design-based inference A survey weight approximates how many population units a sampled unit represents. In its simplest form, the design weight equals the inverse of the selection probability. If a household had a 1 in 500 chance of selection, its basic weight would be 500. In other words, that household represents 500 households in the population.The ICAN-ICAR survey data includes a household weight variable, HH_Weight_Provided, which can be used for population estimate. To compensate for households that were selected but did not respond (or were missed), the weights are adjusted so that responding households “stand in” for similar households that did not respond. This may also involve post-stratification — adjusting weights so that certain known totals (e.g. population by region, or by urban/rural) align with census figures. These adjustments reduce bias from non-response and any sampling frame imperfections. Ignoring survey weights or clustering can make your analysis misrepresent both the population estimate and its uncertainty. If selection probabilities vary, unweighted summaries describe the sample rather than the population, whereas weighting targets the population by rebalancing each observation’s contribution. Clustering also induces correlation within PSUs, thus treating the data as a random sample typically understates standard errors and produces confidence intervals that are too narrow. Survey-aware methods incorporates the design to estimate uncertainty correctly. The svydesign() from the survey package represents the sampling design through a design object. After you create that object, you pass it to survey-aware estimators such as svymean() and svyglm(). In multi-country analysis, cluster and strata identifiers can repeat across countries. The safest approach is to use interaction() so that identifiers become unique across the pooled data. options( survey.lonely.psu = &quot;adjust&quot;, survey.adjust.domain.lonely = TRUE ) des = svydesign( ids = ~interaction(CountryName, VillageID) + HHID, strata = ~interaction(CountryName, Tier_One_Unit), weights = ~HH_Weight_Provided, data = dat, nest = TRUE ) Let’s break down each component: 1. ids = ~interaction(CountryName, VillageID) + HHID This specifies the two-stage cluster structure: Stage 1: EAs/Villages (identified uniquely by interaction(CountryName, VillageID)) Stage 2: Households within EAs (identified by HHID) We use interaction() to create unique EA identifiers across countries, since the same VillageID might exist in multiple countries. The + indicates nested stages (households within EAs) 2. strata = ~interaction(CountryName, Tier_One_Unit) Stratification variables are the geographic regions within each country. Tier_One_Unit represents the first-level administrative division (provinces, districts, etc.). We interact this with CountryName to treat each country as its own separate stratification system. This ensures that sampling variability is calculated correctly within each country’s design 3. weights = ~HH_Weight_Provided This is the final household weight variable provided in the data. Already incorporates all necessary adjustments (selection probabilities, non-response, post-stratification) 4. nest = TRUE This is essential when cluster IDs are only unique within strata. It tells R that VillageID values can repeat across different strata/countries, and prevents R from treating EAs with the same ID in different countries as the same cluster. The example below compares an unweighted and weighted estimate of the proportion of enrolled learners who meet minimum proficiency in both maths and reading in Senegal. # filter to enrolled learners in Senegal one_country = dat |&gt; filter(CountryName == &quot;Senegal&quot; &amp; enr_status == &quot;Currently Enrolled&quot;) # unweighted (sample) estimate prop_unweighted = mean(one_country$MPL_both, na.rm = TRUE) prop_unweighted ## [1] 0.2603826 # weighted estimate with design-correct SE options( survey.lonely.psu = &quot;adjust&quot;, survey.adjust.domain.lonely = TRUE ) des_one = svydesign( ids = ~interaction(CountryName, VillageID) + HHID, strata = ~interaction(CountryName, Tier_One_Unit), weights = ~HH_Weight_Provided, data = one_country, nest = TRUE ) prop_weighted = svymean(~MPL_both, design = des_one, na.rm = TRUE) prop_weighted ## mean SE ## MPL_both 0.26049 0.0109 @fig-pal-analysis-workflow summarizes a practical workflow that you can reuse for descriptive statistics and modelling. Figure 2.2: A practical workflow for analysing ICAN-ICAR survey data in R. After you define des, these are the common survey-aware functions: svymean() for weighted means and proportions, svytotal() for weighted totals, svyby() for subgroup estimates, and svyglm() for regression models with design-correct standard errors. 2.3 Practice exercises Change the country filter in the example and compare unweighted and weighted MPL estimates. Estimate the weighted proportion meeting MPL in maths (MPL_math) and compare it with the unweighted proportion. Within one country, use svyby() to estimate MPL by Location (urban/rural) for enrolled learners. "],["understanding-distributions.html", "Chapter 3 Understanding distributions 3.1 Getting oriented to the data 3.2 Variable types and levels of measurement 3.3 Distributions of continuous variables 3.4 Distributions of categorical variables 3.5 Weighted summaries using the survey design 3.6 Household-level variables in a child-level file 3.7 Missing data and skip patterns 3.8 Exercises 3.9 What logistic regression models 3.10 Simple versus multiple logistic regression 3.11 Data 3.12 Simple Logistic Regression 3.13 Multiple binary logistic regression 3.14 Interaction 3.15 Reporting Results", " Chapter 3 Understanding distributions Large survey datasets contain many variables and many observations. To learn from such data, you need tools that summarise patterns clearly and accurately. This chapter introduces distributions—the way a variable’s values are spread across the sample—and shows how to describe distributions using tables, summary statistics, and simple plots. Because the ICAN-ICAR data come from a complex survey, we also show how to produce weighted summaries that reflect the target population. 3.1 Getting oriented to the data Start by checking the size of the dataset and inspecting the variables. dim(dat) # rows and columns ## [1] 89141 142 names(dat)[1:25] # first 25 variable names ## [1] &quot;CountryName&quot; &quot;Tier_One_Unit&quot; &quot;VillageID&quot; &quot;Location&quot; &quot;HHID&quot; &quot;SubmissionDate&quot; &quot;duration&quot; ## [8] &quot;hh06a&quot; &quot;hh06b&quot; &quot;hh06c&quot; &quot;hh06d&quot; &quot;hh07a&quot; &quot;hh07b&quot; &quot;hh07c&quot; ## [15] &quot;hh07d&quot; &quot;hh07e&quot; &quot;hh07f&quot; &quot;hh07g&quot; &quot;hh07h&quot; &quot;hh07i&quot; &quot;hh07j&quot; ## [22] &quot;hh07k&quot; &quot;hh07l&quot; &quot;hh07m&quot; &quot;hh07n&quot; A useful next step is to focus on a small set of variables. In this course we frequently use child age (ch02), child sex (ch03), enrolment status (enr_status), and assessment timing variables. dat |&gt; select(CountryName, Location, HHID, ChildID, ch02, ch03, ch06a, enr_status, icar_assess_time, ican_assess_time) |&gt; head(10) ## # A tibble: 10 × 10 ## CountryName Location HHID ChildID ch02 ch03 ch06a enr_status icar_assess_time ican_assess_time ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mozambique Urban uuid:f5808f62-6a22-4247-ae68-9eb8… uuid:f… 8 2 4 Currently… 0.783 4.87 ## 2 Mozambique Urban uuid:b98e2157-5941-457c-8249-cea7… uuid:b… 12 2 6 Currently… 5.52 3.33 ## 3 Mozambique Urban uuid:26117653-2d45-474f-a7ea-6fa5… uuid:2… 8 1 3 Currently… 1.73 4.33 ## 4 Mozambique Urban uuid:d46bcfd9-f3e5-44ff-825e-6371… uuid:d… 9 1 5 Currently… 4.22 4.12 ## 5 Mozambique Urban uuid:df927f9a-f03b-480b-ada3-ddfd… uuid:d… 10 2 2 Currently… 0.617 7.38 ## 6 Mozambique Urban uuid:ade5f0d8-768a-479c-956b-976e… uuid:a… 13 1 7 Currently… 9.68 7.2 ## 7 Mozambique Urban uuid:3f84d69b-e298-43d8-8c77-0ae0… uuid:3… 8 1 3 Currently… 4.9 11.3 ## 8 Mozambique Urban uuid:ff7b1d96-0e69-400c-955e-2209… uuid:f… 8 1 3 Currently… 7.28 7.9 ## 9 Mozambique Urban uuid:7730b493-addc-4c60-8a8e-9d31… uuid:7… 16 2 10 Currently… 5.12 10.1 ## 10 Mozambique Urban uuid:e2108c51-8a38-452a-94f5-dd12… uuid:e… 8 2 1 Currently… 6 18 3.2 Variable types and levels of measurement The summary tools you use depend on the type of variable. Continuous (or continuous-like) variables represent quantities. Examples include child age (ch02), household size (hh06a), and assessment time (icar_assess_time). Even when a variable records whole numbers (such as household size), it often behaves like a continuous measure because averages and differences remain interpretable. Categorical variables group observations into distinct categories. Examples include CountryName, Location, and child sex (ch03). For categorical variables, counts and proportions are more informative than averages. The PAL data are stored at the child level: each row is a child. Some variables therefore repeat across children in the same household (for example, household size). When you analyse household-level variables, you must avoid counting the same household multiple times. 3.3 Distributions of continuous variables A distribution summary typically combines (i) a numeric description and (ii) a plot. 3.3.1 Numeric summaries Use summary() for a quick overview, then compute statistics that match your question. summary(dat$ch02) # child age ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 7.00 10.00 10.04 13.00 16.00 summary(dat$icar_assess_time) # ICAR assessment time ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 1.683 4.067 5.569 7.000 675.717 2827 dat |&gt; summarise( mean_age = mean(ch02, na.rm = TRUE), sd_age = sd(ch02, na.rm = TRUE), p25_age = quantile(ch02, 0.25, na.rm = TRUE), median_age = median(ch02, na.rm = TRUE), p75_age = quantile(ch02, 0.75, na.rm = TRUE) ) ## # A tibble: 1 × 5 ## mean_age sd_age p25_age median_age p75_age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.0 3.22 7 10 13 3.3.2 Plots Histograms and density plots show shape (skewness, clustering, and outliers). ggplot(dat, aes(x = ch02)) + geom_histogram(bins = 20) + labs(x = &quot;Child age (years)&quot;, y = &quot;Count&quot;) Figure 3.1: Distribution of child age in the ICAN-ICAR sample. 3.4 Distributions of categorical variables For categorical variables, start with a frequency table and then compute proportions. # raw counts table(dat$ch03, useNA = &quot;ifany&quot;) ## ## 1 2 ## 45213 43928 table(dat$Location, useNA = &quot;ifany&quot;) ## ## Rural Urban ## 49781 39360 It is usually clearer to attach labels. For example, ch03 is typically coded 1/2 for Female/Male. dat = dat |&gt; mutate( sex = factor(ch03, levels = c(1, 2), labels = c(&quot;Female&quot;, &quot;Male&quot;)), residence = factor(Location, levels = c(&quot;Rural&quot;, &quot;Urban&quot;)) ) Now you can create clean frequency tables. sex_tbl = dat |&gt; filter(!is.na(sex)) |&gt; count(sex) |&gt; mutate(percent = 100 * n / sum(n)) kable(sex_tbl, digits = 1, col.names = c(&quot;Sex&quot;, &quot;Count&quot;, &quot;Percent&quot;)) Sex Count Percent Female 45213 50.7 Male 43928 49.3 3.5 Weighted summaries using the survey design Unweighted summaries describe the sample. To describe the population, you must account for weights, clustering, and stratification. We create a survey design object using the same structure used elsewhere in the course. options( survey.lonely.psu = &quot;adjust&quot;, survey.adjust.domain.lonely = TRUE ) des_child_all = svydesign( ids = ~interaction(CountryName, VillageID) + HHID, strata = ~interaction(CountryName, Tier_One_Unit), weights = ~HH_Weight_Provided, data = dat, nest = TRUE ) 3.5.1 Weighted proportions and means A weighted proportion of girls in the population: svymean(~I(ch03 == 1), des_child_all, na.rm = TRUE) ## mean SE ## I(ch03 == 1)FALSE 0.51713 0.0032 ## I(ch03 == 1)TRUE 0.48287 0.0032 Weighted means for several continuous variables: svymean(~ch02 + icar_assess_time + ican_assess_time, des_child_all, na.rm = TRUE) ## mean SE ## ch02 10.0037 0.0238 ## icar_assess_time 5.8671 0.1045 ## ican_assess_time 7.7783 0.1302 3.5.2 Weighted frequency tables Weighted distributions use svytable(). country_w = svytable(~CountryName, des_child_all) country_w_tbl = tibble( CountryName = names(country_w), weighted_n = as.numeric(country_w), percent = as.numeric(100 * prop.table(country_w)) ) kable(country_w_tbl, digits = 1, col.names = c(&quot;Country&quot;, &quot;Weighted count&quot;, &quot;Percent&quot;)) Country Weighted count Percent Bangladesh 46534652 17.9 Kenya 18133333 7.0 Mali 3703030 1.4 Mexico 51275864 19.7 Mozambique 9694001 3.7 Nepal 16370232 6.3 Nicaragua 1132365 0.4 Pakistan 53312701 20.5 Senegal 4097044 1.6 Tanzania 24984302 9.6 Uganda 31361665 12.0 3.6 Household-level variables in a child-level file When a household-level variable repeats across children, a child-level frequency table over-weights large households. A simple fix is to keep one record per household. dat = dat |&gt; arrange(HHID, ChildID) |&gt; group_by(HHID) |&gt; mutate(hh_first = row_number() == 1) |&gt; ungroup() For example, compare a child-level and household-level distribution of roof type (hh07a), if that variable is available in your extract. # child-level distribution (counts children) dat |&gt; count(hh07a) |&gt; mutate(percent = 100 * n / sum(n)) # household-level distribution (counts households once) dat |&gt; filter(hh_first) |&gt; count(hh07a) |&gt; mutate(percent = 100 * n / sum(n)) To produce a weighted household-level distribution, subset the survey design to one row per household. des_household = subset(des_child_all, hh_first) roof_w = svytable(~hh07a, des_household) tibble( roof_type = names(roof_w), percent = as.numeric(100 * prop.table(roof_w)) ) 3.7 Missing data and skip patterns Missing values are common in survey data. In PAL, missingness often reflects skip patterns. For example, grade (ch06a) is usually recorded only for learners who are currently enrolled. Start by checking how much is missing. dat |&gt; summarise( total = n(), missing_ch06a = sum(is.na(ch06a)), missing_pct = 100 * sum(is.na(ch06a)) / n() ) ## # A tibble: 1 × 3 ## total missing_ch06a missing_pct ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 89141 9915 11.1 Then evaluate whether missingness aligns with enrolment status. dat |&gt; group_by(enr_status) |&gt; summarise( total = n(), missing_ch06a = sum(is.na(ch06a)), missing_pct = 100 * sum(is.na(ch06a)) / n() ) |&gt; arrange(desc(total)) ## # A tibble: 3 × 4 ## enr_status total missing_ch06a missing_pct ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Currently Enrolled 79397 171 0.215 ## 2 Never Enrolled 7517 7517 100 ## 3 Out of School 2227 2227 100 You can also estimate weighted missingness rates by subgroup. svyby( ~I(is.na(ch06a)), ~enr_status, des_child_all, svymean, na.rm = TRUE ) ## enr_status I(is.na(ch06a))FALSE I(is.na(ch06a))TRUE se.I(is.na(ch06a))FALSE se.I(is.na(ch06a))TRUE ## Currently Enrolled Currently Enrolled 0.9991602 0.0008397509 0.0001091877 0.0001091877 ## Never Enrolled Never Enrolled 0.0000000 1.0000000000 0.0000000000 0.0000000000 ## Out of School Out of School 0.0000000 1.0000000000 0.0000000000 0.0000000000 3.8 Exercises Use the PAL data and the tools in this chapter to answer the questions below. What proportion of the sample is rural (Location == \"Rural\")? Compute both the unweighted and weighted estimate. What is the unweighted and weighted mean child age (ch02) for enrolled learners (enr_status == \"Currently Enrolled\")? Create an age group variable (for example, &lt;= 10 vs &gt; 10). What share of learners falls in each age group? Produce a weighted frequency table for Location within one country of your choice. Investigate missingness in ch06a by residence and by sex. Report the results as unweighted and weighted proportions. # Example template: weighted rural proportion svymean(~I(Location == &quot;Rural&quot;), des_child_all, na.rm = TRUE) &lt;!--chapter:end:02-understanding-distributions.Rmd--&gt; # Logistic Regression Many questions in early-foundational learning research ask about the likelihood of a binary outcome - something that is either *&quot;yes/no&quot;* or *&quot;0/1&quot;*. For instance, how likely is a child to meet the minimum proficiency level in both maths and reading, given the child&#39;s age and whether the language of assessment matches with the child&#39;s home language. In this lesson, we use the PAL Network&#39;s ICAN-ICAR 2025 survey data to model binary outcomes using logistic regression. A simple linear regression can be used as a “linear probability model”, but it often produces predicted values below 0 or above 1 and does not handle the changing variability that comes with binary outcomes. Logistic regression avoids these issues by modeling the log-odds of the outcome, which naturally maps to predicted probabilities between 0 and 1. Because ICAN/ICAR is collected using a complex survey design (stratification, clustering, and survey weights), we fit models using the survey package: we first define the design with svydesign() and then estimate logistic regression with svyglm() so that standard errors and inference reflect the sampling design. 3.9 What logistic regression models Suppose \\(Y\\) indicates whether a learner meets the MPL in maths, where \\(Y = 1\\) means “meets MPL” and \\(Y = 0\\) means “does not meet MPL”. Let \\(X_1\\) be ICAN assessment time for maths. Figure 3.2: Linear regression of a binary Y variable (0 = learner do not meet MPL for Maths, 1 = learner meets MPL for Maths) on a measurement X variable (child’s age in years). A linear regression of a binary outcome is sometimes called a linear probability model. It can produce predicted values outside the \\([0,1]\\) range. For example in @fig-simplereg, \\[\\hat{Y} = -0.351 + 0.0859X_1,\\]predictions exceed 1 when \\(X_1 &gt; 15.8\\). This is not meaningful because probabilities must lie between 0 and 1. In addition, with a binary outcome the variability changes with the mean (because \\(Var(Y |X) = p(1- p)\\)), so the constant-variance assumption behind ordinary linear regression is not appropriate here. Logistic regression starts by modeling the probability of “success”: \\[ p = P(Y = 1 | X). \\] Since \\(Y\\) is binary, \\(P(Y = 0 | X) = 1-p\\). The odds of meeting MPL are defined as \\[\\text{odds} = \\frac{p}{1 - p}.\\] Odds range from 0 to \\(\\infty\\). For example, if \\(p = 0.2\\), then the odds are \\(0.2/0.8 = 0.25\\). You can say odds are 0.25 to 1,or 1 to 4. Logistic regression assumes the log-odds (the logit) are linearly related with the predictors: \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 \\] Solving for \\(p\\) gives the logistic curve: \\[ p = \\frac{\\text{exp}(\\beta_0 + \\beta_1X_1)}{1 + \\text{exp}(\\beta_0 + \\beta_1X_1)}, \\] which always stays between 0 and 1. Here, \\(\\beta_1\\) is the change in the log-odds per one-unit increase in \\(X_1\\), and \\(\\text{exp}(\\beta_1)\\) is the odds ratio associated with a one-unit increase in \\(X_1\\). If the logit is a linearly related to the \\(X\\) variables, then the probability \\(p\\) is a non-linear, S-shaped function with respect to the \\(X\\) variables as in @fig-logreg. Figure 3.3: Predicted probability as a logit function X Logistic regression is widely used in many fields whenever we want to predict the probability of a binary outcome (yes/no, 0/1). For example: Medicine: to estimate the probability that a patient has (or will develop) a disease using medical history and test results. Banking/Finance: to estimate the probability that a borrower will default, based on income, credit history, and other profile information. Politics: to estimate the probability that a citizen will vote for a particular party or candidate. Many others: marketing (purchase vs not), education (pass vs fail), and program evaluation (participate vs not). In this lesson, we apply logistic regression to ICAN/ICAR survey data to model whether a learner meets the Minimum Proficiency Level (MPL). 3.10 Simple versus multiple logistic regression Binary logistic regression has an outcome \\(Y\\) which has two categories (e.g., \\(Y =1\\), \\(Y = 0\\)). We distinguish between two common forms: Simple logistic regression uses one predictor: \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0+\\beta_1X_1 \\] Multiple logistic regression uses two or more predictors: \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\ldots+ \\beta_kX_k \\] Multiple logistic regression is often more useful because it lets us adjust for other factors. Each coefficient describes the relationship between a predictor and the outcome holding the other predictors constant. Because the ICAN/ICAR data come from a complex survey design, we estimate both simple and multiple logistic regression models using svmglm() with a design object created by svydesign(). 3.11 Data We create an analysis-ready data by, (i) restricting to a specific country and learner subgroup, (ii) selecting a small set of variables needed for the models, and (iii) re-coding key variables into analysis-friendly formats (especially binary predictors and the MPL outcome). ch02: Child’s age ch03: Child’s sex ch09: Has the child brought any material to read (other than school textbooks) from the school library MPL_both: Whether the learner meets minimum proficiency level in both maths and reading CountryName: Unique country name Tier_One_Unit: Geographical divisions within one country (county, state, province, etc) VillageID: Unique village identifiers HHID: Unique household identifier 3.11.1 Import, subset, and keep only needed variables # load packages library(tidyverse) library(survey) library(skimr) # load the data dat = read_csv(&quot;data/2025_PAL_ICAN-ICAR_data.csv&quot;) |&gt; filter( CountryName == &quot;Senegal&quot; &amp; # restrict to this country enr_status == &quot;Currently Enrolled&quot; # currently enrolled in school ) |&gt; select( Location, CountryName, Tier_One_Unit, VillageID, HHID, HH_Weight_Provided, ch02, ch03, ch09, MPL_both ) |&gt; rename( age = ch02, sex = ch03, books = ch09, mpl_both = MPL_both ) 3.11.2 Recode variables (labels + reference levels) dat = dat |&gt; mutate( # Outcome (0/1 -&gt; No/Yes) mpl_both = factor(mpl_both, levels = c(0,1), labels = c(&quot;No&quot;, &quot;Yes&quot;)), # Binary predictors (0/1 -&gt; No/Yes) books = factor(books, levels = c(0,1), labels = c(&quot;No&quot;, &quot;Yes&quot;)), sex = factor(sex, levels = c(1,2), labels = c(&quot;Female&quot;, &quot;Male&quot;)), Location = factor(Location) ) Always recognize the order of the levels for the dependent variable because it will have an impact on the interpretations. In R, the first level from levels() is always taken as the reference level. In our case, the first level is not meeting the MPL and the second level is meeting the MPL for both maths and reading. levels(dat$mpl_both) ## [1] &quot;No&quot; &quot;Yes&quot; This means that when we estimate the impact of the independent variable(s) it will be on the learner meeting the MPL (and not on the learner not meeting the MPL). Here is the preview of the data and some descriptive statistics. dat |&gt; skim() Table 3.1: Data summary Name dat Number of rows 6429 Number of columns 10 _______________________ Column type frequency: character 4 factor 4 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace CountryName 0 1 7 7 0 1 0 Tier_One_Unit 0 1 5 11 0 14 0 VillageID 0 1 4 4 0 201 0 HHID 0 1 41 41 0 3399 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Location 0 1 FALSE 2 Urb: 4021, Rur: 2408 sex 0 1 FALSE 2 Fem: 3765, Mal: 2664 books 0 1 FALSE 2 No: 5285, Yes: 1144 mpl_both 0 1 FALSE 2 No: 4755, Yes: 1674 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist HH_Weight_Provided 0 1 504.76 171.05 183.02 399.55 508.64 535.41 1262.94 ▃▇▁▁▁ age 0 1 10.58 3.11 5.00 8.00 11.00 13.00 16.00 ▇▇▇▇▇ 3.11.3 Why we need survey design information The ICAN/ICAR data come from a complex, multi-stage sample (clustered and stratified). To get correct standard errors, we create a survey design object that tells R which variables represent strata, primary sampling units (PSUs), and sampling weights. After this step, we fit models using svyglm() with the design object rather than using glm() directly. options( survey.lonely.psu = &quot;adjust&quot;, survey.adjust.domain.lonely = TRUE ) des = svydesign( ids = ~interaction(CountryName, VillageID) + HHID, strata = ~interaction(CountryName, Tier_One_Unit), weights= ~HH_Weight_Provided, nest = TRUE, data = dat ) 3.12 Simple Logistic Regression A simple logistic regression models a binary outcome using one predictor. We will illustrate simple logistic regression with: a continuous predictor, and a categorical predictor. 3.12.1 Continuous predictor: child’s age Suppose we want to model whether a learner meets MPL for both maths and reading (MPL_both) using the the child’s age (age). Here. m1 = svyglm(mpl_both ~ age, family = quasibinomial, design =des, na.action = na.omit ) m1 |&gt; tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5.50 0.184 -29.9 5.37e-73 ## 2 age 0.391 0.0145 26.9 5.50e-66 3.12.1.1 Interpreting the coefficient (log-odds) In a logistic model, \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0+\\beta_1X \\] so \\(\\beta_1\\) is the change in log-odds for a one-unit increase in \\(X\\) (here, one more year in child’s age). If \\(\\beta_1 &gt; 0\\): odds (and probability) of meeting MPL for both tends to increase as \\(X\\) increases. If \\(\\beta_1 &lt; 0\\): odds (and probability) of meeting MPL for both tend to decrease as \\(X\\) increases. Here, \\(\\hat{\\beta_1} \\approx 0.39\\), meaning that one year increase in child’s age leads to \\(0.39\\) increase in the log odds of a child meeting MPL for both maths and reading. Older children are more likely in odds terms to meet MPL for both maths and reading, holding the model as specified. 3.12.1.2 Interpreting the odds ratio Exponentiation of the \\(\\hat{\\beta_1}\\) coefficient converts back to the odds ratio of a child meeting MPL for both \\((Y = 1)\\). exp(coef(m1)[&quot;age&quot;]) ## age ## 1.478525 We can say that an additional year in the child’s age multiplies odds of meeting MPL by \\(~1.48\\). This means that the odds of meeting MPL for both maths and reading are about \\(48\\%\\) higher per additional year in the child’s age (because \\(1.4785 - 1 \\approx 0.4785\\)). Also, R performs a hypothesis test for each coefficient, that is \\(H_0: \\beta_j = 0\\) versus \\(H_0: \\beta_j \\neq 0\\) for \\(j = [0, 1]\\) via the Wald test, and prints the p-values in the last column. We can thus compare these p-values to the chosen significance level (usually \\(\\alpha = 0.05\\)) to conclude whether or not each of the coefficient is significantly different from zero. The lower the p-value, the more evidence that the coefficient is different from zero. This is similar to the linear regression. Here, the p-value \\(&lt; 0.05\\). This means that there is sufficient evidence that the age coefficient is statistically different from zero at the \\(5\\%\\) significance level. 3.12.1.3 Predicted probabilities A good next step is to use the model to compute the predicted probabilities for few ages. Suppose we would like to predict the probability of meeting MPL for both maths and reading for a child that is aged \\(7, 10, 13, 16\\): newdat = data.frame(age = c(7, 10, 13, 16)) # get predictions on the link (logit) scale pred = predict(m1, newdata = newdat) newdat$probabilities = plogis(pred) #convert logit --&gt; probability newdat ## age probabilities ## 1 7 0.05960664 ## 2 10 0.17003253 ## 3 13 0.39836984 ## 4 16 0.68154314 As the child grows in years the likelihood of them meeting the MPL for both increases substantially. We can also visualize the results of our model below: library(sjPlot) # plot plot_model( m1, type = &quot;pred&quot;, terms= &quot;age&quot; ) + labs(y = &quot;Probability of MPL for both maths &amp; reading&quot;, x = &quot;Child&#39;s Age in Years&quot;, title = &quot;Predicted Probabilities of MPL for both Maths &amp; Reading using Age&quot;) + theme_classic() The above plot shows that probability of a child meeting MPL for both reading and maths increases with the child’s age. 3.12.2 Categorical predictor example: Books Suppose we are now interested in predicting the probability of a child meeting MPL for both using whether the child has brought home any material (other than textbooks) to read from the school library (books). Recall that when the predictor was continuous, \\(e^{\\hat{\\beta_1}}\\) was the multiplicative change in the odds in favor of \\(Y = 1\\) as \\(X_1\\) increases by one unit. With \\(X_1\\) being books, a categorical variable, the only increase possible is 0 to 1 (or from 1 to 2 if sex is encoded as a factor). So, we can write the interpretation in terms of Yes/No: \\(e^{\\hat{\\beta_1}}\\) is the multiplicative change of the odds in favor of \\(Y = 1\\) as No becomes Yes, keeping in mind the order of the level for the variable books. # levels for books variable levels(dat$books) ## [1] &quot;No&quot; &quot;Yes&quot; So, it is indeed the multiplicative change of the odds in favor of \\(Y = 1\\) as No becomes Yes. If the level No came before the level Yes in the data, it would have been the opposite. We fit the model again: m2 = svyglm(mpl_both ~ books, family = quasibinomial, design =des, na.action = na.omit ) m2 |&gt; tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.14 0.0635 -17.9 2.18e-42 ## 2 booksYes 0.492 0.0941 5.23 4.60e- 7 3.12.2.1 Interpreting the coefficient (log-odds) As the No becomes Yes, the log-odds of meeting MPL for both changes by \\(0.492\\). The odds (and probabilities) tend to increase as the child start to bring back materials to read from the school library. 3.12.2.2 Interpreting the odds ratio Odds ratio for books variable are exp(coef(m2)[&quot;booksYes&quot;]) ## booksYes ## 1.635037 For children that bring home study materials from the school library, their odds of meeting MPL is multiplied by a factor of \\(e^{0.492} = 1.635\\) relative to those who do not bring back home study materials from the school library. This means that the odds of meeting MPL for both reading and maths are \\((1.635 -1)*100 = 63.5\\%\\) higher for those children who bring home study materials from the school library than those who do not bring back home study materials. The interpretation of the intercept \\(\\hat{\\beta_0} = -1.138\\) gives the probability of a child meeting MPL when \\(X= 0\\). In our case, this simply maps to those children who do not bring back study materials from the school library. Therefore, the probability of a a child who does not bring back home study material meeting MPL for both reading and math is: exp(coef(m2)[1]) / (1+exp(coef(m2)[1])) ## (Intercept) ## 0.2426628 The model’s intercept tells us that the probability of a child that does not bring back home study materials from the school library meeting MPL for both is only \\(24,2\\%\\). 3.12.2.3 Predicted probabilities As with the continuous predictor, predictions can also be made with the predict() function. Suppose we want to predict the probability of a child who brings back home study material from the school library meeting MPL for both reading and math: # predict probability to meet MPL newdat = data.frame(books = c(&quot;Yes&quot;, &quot;No&quot;)) pred = predict(m2, newdata = newdat, type = &quot;link&quot; ) newdat$probabilities = plogis(pred) newdat ## books probabilities ## 1 Yes 0.3437854 ## 2 No 0.2426628 Based on this model, it is predicted that a child who brings back home study material from the school library has a \\(34,4\\%\\) chance of meeting MPL for both reading and math. This is higher relative to those children who do not bring back home study materials. We can also visualize these results using the plot_model function: # plot plot_model(m2, type = &quot;pred&quot;, terms = &quot;books&quot; ) + labs(y = &quot;Probability of MPL for both maths &amp; reading&quot;, title = &quot;Predicted Probabilities of MPL for both Maths &amp; Reading using Books&quot; ) + theme_classic() The points correspond to the predicted probabilities, and the bars correspond to their confidence intervals. 3.13 Multiple binary logistic regression The interpretation of the coefficients in multiple logistic regression is similar to that of simple logistic regression, except that this time it estimates the multiplicative change in the log-odds in favor of \\(Y = 1\\) when \\(X\\) increases by one unit, while holding other predictors constant. For this illustration, suppose we want to predict the MPL for both reading and math using age. sex, and books. # fit the model m3 = svyglm(mpl_both ~ age + sex + books, family = quasibinomial, design =des, na.action = na.omit ) # print results m3 |&gt; tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5.53 0.181 -30.6 4.76e-74 ## 2 age 0.389 0.0145 26.9 1.41e-65 ## 3 sexMale -0.0455 0.0705 -0.646 5.19e- 1 ## 4 booksYes 0.393 0.105 3.74 2.46e- 4 Based on the p-values, we can conclude that, at the \\(5\\%\\) significance level, only age and books are significantly associated with MPL for both reading and math (p-value \\(&lt; 0.05\\)). Similar to simple logistic regression, it is easier to interpret these relationships through the odds ratios. But this time, we also print \\(95\\%\\) CI of the odds ratios in addition to the OR (rounded to 3 decimals) so that we can easily see which ones are significantly different from 1. If \\(95\\%\\) CI include the value of 1 in their bound, then there is no significant association between the MPL both and the predictor in question. This is another way of confirming the significance of a predictor. # OR and 95% CI round(exp(cbind(OR = coef(m3), confint(m3))), 3) ## OR 2.5 % 97.5 % ## (Intercept) 0.004 0.003 0.006 ## age 1.476 1.434 1.519 ## sexMale 0.956 0.831 1.098 ## booksYes 1.482 1.204 1.824 From the OR and their \\(95\\%\\) CI computed above, we can conclude that: age: an additional year in the child’s age multiplies odds of meeting MPL by \\(~1.48\\).This means that the odds of meeting MPL for both maths and reading are about \\(48\\%\\) higher per additional year in the child’s age, ceteris paribus. books: the odds of a child that brings back home study material from the school library meeting MPL for both reading and math are \\(1.482\\) times higher than the child who does not bring back home study material, ceteris paribus. We refrain from interpreting the sex as it not significant at the \\(5\\%\\) significance level (also, 1 is included in its \\(95\\%\\) CI). For illustrative purposes, suppose now we want to predict the probability that a new child meets MPL for both reading and math. Suppose that this child is from Senegal, aged 15 years old, male, and does not bring back home study material from the school library: # create data frame of new learner new_learner = data.frame( age = 15, sex = &quot;Male&quot;, books = &quot;No&quot; ) # predict probability to meet MPL both pred = predict(m3, newdata = new_learner, type = &quot;link&quot; ) # print results new_learner$probabilities = plogis(pred) new_learner ## age sex books probabilities ## 1 15 Male No 0.5646007 If we trust our logistic regression model, the probability that this new child meets MPL for both reading and math is predicted to be \\(56.5\\%\\). We can also visualize the effect of age and books on the predicted probability of meeting MPL in @fig-vistwo. plot_model(m3, type = &quot;pred&quot;, terms = c(&quot;age&quot;, &quot;books&quot;) ) + labs(y = &quot;Prob(MPL on both reading and math)&quot;, x = &quot;Child&#39;s Age in Years&quot;, title = &quot;Predicted probabilities of MPL with Age and Books&quot;) + theme_classic() These plots confirm that: There is a positive relationship between the child’s age and meeting MPL for both maths and reading. The odds of meeting MPL for both maths and reading is higher for children who bring back home study material from the school library for reading. This observation intensifies as the child’s age increases. 3.14 Interaction In the last section, we never considered potential interaction effects. In a regression model, interaction occurs when the relationship between a predictor and the outcome depends on the value or the level taken by another predictor. On the contrary, if the relationship between a predictor and the outcome remains unchanged no matter the value taken by another predictor, we cannot conclude that there is an interaction effect. In our case, there would be an interaction if the relationship between the child’s age and MPL for both maths and reading depends on the sex predictor. There would be an interaction, for instance, if the relationship between the child’s age and MPL for both maths and reading was positive for female children, and negative for male children, or vice versa. Or if the relationship between the child’s age and MPL for both maths and reading was much stronger or much weaker for female children than for male children. Let us see if there is any interaction between age and sex, and more importantly, whether or not this interaction is significant: # save model with interaction m4_inter = svyglm(mpl_both ~ age + sex + books + sex*age, family = quasibinomial, design =des, na.action = na.omit ) m4_inter |&gt; tidy() ## # A tibble: 5 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5.84 0.245 -23.8 5.81e-58 ## 2 age 0.415 0.0197 21.1 8.96e-51 ## 3 sexMale 0.666 0.278 2.40 1.76e- 2 ## 4 booksYes 0.397 0.105 3.78 2.15e- 4 ## 5 age:sexMale -0.0591 0.0234 -2.53 1.24e- 2 3.14.1 Interpretation of age, sex, and the age*sex interaction age is positively associated with MPL (\\(\\beta_{age} = 0.415\\)). For the reference group (Females), this corresponds to an odds ratio (OR) of \\(1.51\\) per one-year increase in age (OR $ = e^{0.415} = 1.514$), holding other predictors constant. The age-by-sex interaction is statistically significant (\\(\\beta_{int} = -0.059\\), p-value\\(=0.012\\)), which indicates that the age effect differs by sex. Specifically, the age slope for males equals \\(\\beta_{age}+ \\beta_{int} = 0.355\\), which corresponds to an OR of \\(1.43\\) per year (\\(e^{0.355} = 1.427\\)). Equivalently, the male-to-female odds ratio changes multiplicatively by \\(e^{\\beta_{int}} = 0.943\\) per additional year of age (\\(\\approx 5,7\\%\\) decrease per year), which implies that the relative male-female difference diminishes with increasing age. Importantly, in the presence of an interaction, the main-effect coefficient for sex (\\(\\beta_{sexMale} = 0,666\\)) represents the male-female contrast when age is \\(0\\) years old. Consequently, \\(\\beta_{sexMale}\\) should not be interpreted as an average sex effect across the observed age range. Next, let us visualize the age* sex interaction in @fig-visthree. plot_model(m4_inter, type = &quot;pred&quot;, terms = c(&quot;age&quot;, &quot;sex&quot;) ) + labs(y = &quot;Prob(MPL on both reading and math)&quot;, x = &quot;Child&#39;s Age in Years&quot;, title = &quot;&quot;) + theme_classic() Figure 3.4: Predicted probability of MPL as a function of age, stratified by sex, from a logistic regression including an age×sex interaction (bands denote 95% confidence intervals). Predictions are computed with other predictors held constant (e.g., books fixed at its reference level). Predicted probabilities from the interaction model increase with age for both sexes, which is consistent with the positive age coefficient. However, due to the negative age*sex interaction, the increase is steeper for females than for males, which leads to an age-dependent sex difference and a cross-over in the predicted curves around ~ 11 years. Confidence bands widen at the extremes of age, which reflects increased uncertainty where data is sparser. Note: When plotting predicted probabilities as a function of age and sex, other predictors (i.e., books) are held constant. 3.15 Reporting Results As we have seen before, odds ratios are useful when reporting results of binary logistic regressions. Computing these odds ratios together with the confidence intervals is not particularly difficult. However, presenting them in a table for a publication or a report can quickly become time consuming, in particular if you have many models and many independent variables. Luckily, there are two packages which saved me a lot of time and which I use almost every time I need to report results of a logistic regression. It is knitr() and kableExtra(). Here is an example with one of the models we have built previously: # load the package library(knitr) library(kableExtra) # print table of results tab = summary(m4_inter)$coefficients |&gt; as.data.frame() |&gt; tibble::rownames_to_column(&quot;term&quot;) |&gt; rename( estimate = Estimate, std.error = `Std. Error`, statistic = `t value`, p_value = `Pr(&gt;|t|)` ) |&gt; mutate( OR = exp(estimate), OR_low = exp(estimate - 1.96 * std.error), OR_high = exp(estimate + 1.96 * std.error), p_value = ifelse(p_value &lt; 0.001, &quot;&lt;0.001&quot;, sprintf(&quot;%.3f&quot;, p_value)), `OR (95% CI)` = sprintf(&quot;%.2f (%.2f, %.2f)&quot;, OR, OR_low, OR_high) ) |&gt; select(term, estimate, std.error, statistic, `OR (95% CI)`, p_value) # print the results kable( tab, booktabs = TRUE, align = c(&quot;l&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;), digits = c(NA, 3, 3, 2, NA, NA), caption = &quot;Survey-weighted logistic regression (svyglm) results&quot; ) |&gt; kable_styling( full_width = FALSE, position = &quot;left&quot;, latex_options = c(&quot;hold_position&quot;) # nice for PDF/Quarto/Rmd ) |&gt; add_header_above(c(&quot; &quot; = 1, &quot;Coefficient&quot; = 3, &quot;Odds ratio&quot; = 1, &quot; &quot; = 1)) |&gt; column_spec(1, bold = TRUE) (#tab:reporting results)(#tab:reporting results)Survey-weighted logistic regression (svyglm) results Coefficient Odds ratio term estimate std.error statistic OR (95% CI) p_value (Intercept) -5.840 0.245 -23.82 0.00 (0.00, 0.00) &lt;0.001 age 0.415 0.020 21.05 1.51 (1.46, 1.57) &lt;0.001 sexMale 0.666 0.278 2.40 1.95 (1.13, 3.36) 0.018 booksYes 0.397 0.105 3.78 1.49 (1.21, 1.83) &lt;0.001 age:sexMale -0.059 0.023 -2.53 0.94 (0.90, 0.99) 0.012 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
