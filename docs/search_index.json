[["index.html", "Chapter 1 Introduction", " AFLEARN Introduction to R using ICAN-ICAR 2025 Survey Data Siphiwe M. Bogatsu February 2026 Chapter 1 Introduction "],["getting-to-know-your-data.html", "Chapter 2 Getting to Know Your Data 2.1 Learning Objectives 2.2 Setup 2.3 Understanding Data Structure and Dimensions 2.4 Quick Data Structure Overview 2.5 First Look at the Data 2.6 Variable Names Exploration 2.7 Checking Variable Types 2.8 Exercise 1.1: Exploring Specific Rows 2.9 Exercise 1.2: Exploring Variable Types", " Chapter 2 Getting to Know Your Data 2.1 Learning Objectives By the end of this session, you will be able to: Load and inspect the structure of the PAL ICAN/ICAR dataset Understand the dimensions and organization of the data Explore variable names, types, and their correspondence to the codebook Conduct basic data quality checks Create simple data summaries for different variable types In the previous session, we learned how the ICAN–ICAR survey data was sampled and created a survey design object that allows us to make population-level estimates. Before we analyse any outcomes, we must first understand what the data actually contains, how it is structured, and whether it is ready for analysis. We assume that a valid survey design object has already been created. This session focuses on understanding the underlying data that feed into that design.} This session focuses on — not inference. The ICAN-ICAR 2025 survey is a foundational learning assessment focused on children aged 5-16 years. The survey collects data on: child demographics and enrollment status, household characteristics and assets, literacy and numeracy assessment results, and parental information 2.2 Setup # Load required packages library(tidyverse) # Data manipulation and visualization ## Warning: package &#39;tidyverse&#39; was built under R version 4.5.2 ## Warning: package &#39;ggplot2&#39; was built under R version 4.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.5.2 ## Warning: package &#39;tidyr&#39; was built under R version 4.5.1 ## Warning: package &#39;purrr&#39; was built under R version 4.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 4.5.1 ## Warning: package &#39;stringr&#39; was built under R version 4.5.1 ## Warning: package &#39;lubridate&#39; was built under R version 4.5.1 library(readr) # Reading CSV files library(skimr) # Quick data summaries ## Warning: package &#39;skimr&#39; was built under R version 4.5.1 library(janitor) # Data cleaning helpers ## Warning: package &#39;janitor&#39; was built under R version 4.5.1 library(survey) # Survey analysis ## Warning: package &#39;survey&#39; was built under R version 4.5.1 # Load the PAL ICAN/ICAR dataset data_weighted &lt;- read_csv(&quot;data/2025_PAL_ICAN-ICAR_data.csv&quot;) 2.3 Understanding Data Structure and Dimensions Core Question: What is the shape of this data? # How many rows (observations) and columns (variables)? nrow(data_weighted) ## [1] 89141 ncol(data_weighted) ## [1] 142 We have 89141 observations spread across 142 variables. Each row represents a child (aged 5-16 years) nested within a household, village, and country. Each column represents a survey variable. Note that the sampling was done at the household level, but learning assessments were conducted for all eligible children within selected households. This means multiple children can come from the same household. 2.4 Quick Data Structure Overview # Get a glimpse of the data structure glimpse(data_weighted) ## Rows: 89,141 ## Columns: 142 ## $ CountryName &lt;chr&gt; &quot;Mozambique&quot;, &quot;Mozambique&quot;, &quot;Mozambique&quot;, &quot;Mozamb… ## $ Tier_One_Unit &lt;chr&gt; &quot;Niassa&quot;, &quot;Niassa&quot;, &quot;Niassa&quot;, &quot;Niassa&quot;, &quot;Niassa&quot;,… ## $ VillageID &lt;chr&gt; &quot;c101&quot;, &quot;c101&quot;, &quot;c101&quot;, &quot;c101&quot;, &quot;c101&quot;, &quot;c101&quot;, &quot;… ## $ Location &lt;chr&gt; &quot;Urban&quot;, &quot;Urban&quot;, &quot;Urban&quot;, &quot;Urban&quot;, &quot;Urban&quot;, &quot;Urb… ## $ HHID &lt;chr&gt; &quot;uuid:f5808f62-6a22-4247-ae68-9eb891657449&quot;, &quot;uui… ## $ SubmissionDate &lt;chr&gt; &quot;9/23/2025, 5:02:46 PM&quot;, &quot;9/23/2025, 5:02:47 PM&quot;,… ## $ duration &lt;dbl&gt; 979, 986, 2167, 2078, 1251, 2199, 2889, 1588, 220… ## $ hh06a &lt;dbl&gt; 4, 7, 6, 6, 5, 5, 5, 4, 4, 5, 7, 9, 5, 13, 5, 6, … ## $ hh06b &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hh06c &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1… ## $ hh06d &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hh07a &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2… ## $ hh07b &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2… ## $ hh07c &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hh07d &lt;dbl&gt; 1, 1, 1, 4, 4, 4, 1, 1, 4, 4, 3, 4, 7, 3, 1, 4, 1… ## $ hh07e &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hh07f &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hh07g &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1… ## $ hh07h &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1… ## $ hh07i &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1… ## $ hh07j &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1… ## $ hh07k &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1… ## $ hh07l &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0… ## $ hh07m &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hh07n &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, … ## $ hh07o &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hh07p &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0… ## $ hh07p_3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ hh07q &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ChildID &lt;chr&gt; &quot;uuid:f5808f62-6a22-4247-ae68-9eb891657449_child1… ## $ ch02 &lt;dbl&gt; 8, 12, 8, 9, 10, 13, 8, 8, 16, 8, 7, 9, 10, 7, 11… ## $ ch03 &lt;dbl&gt; 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2… ## $ ch04a &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ ch04b &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2… ## $ ch04c &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ ch04d &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ ch04e &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ ch04f &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1… ## $ ch05 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1… ## $ ch06a &lt;dbl&gt; 4, 6, 3, 5, 2, 7, 3, 3, 10, 1, 2, 2, 4, 2, NA, 8,… ## $ ch06b &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, … ## $ ch06c &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, … ## $ ch06d &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, NA, 1, … ## $ ch06e &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, … ## $ ch07a &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ch07b &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ch07c &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ch07d &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ch08 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1… ## $ ch09 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1… ## $ ch10a &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, NA, 1, … ## $ ch10b &lt;dbl&gt; 2, 3, 4, NA, NA, NA, NA, 1, 1, 1, 7, NA, 7, 7, NA… ## $ ch10c &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 2, 2, 0… ## $ pt00 &lt;dbl&gt; 1, 1, 3, 3, 3, 0, 3, 3, 3, 2, 0, 0, 1, 0, 3, 1, 0… ## $ pt01b &lt;dbl&gt; 28, 40, 47, 26, 36, NA, 52, 26, 36, NA, NA, NA, 2… ## $ pt01c &lt;dbl&gt; 1, 1, 1, 0, 0, NA, 1, 1, 0, NA, NA, NA, 1, NA, 0,… ## $ pt01d &lt;dbl&gt; 1, 1, 1, NA, NA, NA, 1, 1, NA, NA, NA, NA, 1, NA,… ## $ pt01e &lt;dbl&gt; 0, 0, 0, 0, 0, NA, 0, 1, 0, NA, NA, NA, 1, NA, 0,… ## $ pt01f &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 4, NA, NA, NA, NA, 3,… ## $ pt02b &lt;dbl&gt; NA, NA, 63, 30, 50, NA, 59, 31, 43, 40, NA, NA, N… ## $ pt02c &lt;dbl&gt; NA, NA, 1, 1, 1, NA, 1, 1, 1, 1, NA, NA, NA, NA, … ## $ pt02d &lt;dbl&gt; NA, NA, 2, 2, 1, NA, 2, 2, 2, 2, NA, NA, NA, NA, … ## $ pt02e &lt;dbl&gt; NA, NA, 1, 1, 1, NA, 1, 1, 1, 1, NA, NA, NA, NA, … ## $ pt02f &lt;dbl&gt; NA, NA, 4, 4, 3, NA, 4, 4, 4, 1, NA, NA, NA, NA, … ## $ assessment &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ sample &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1… ## $ l1.1 &lt;dbl&gt; 1, 2, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 0, 0, 2, 1, 2… ## $ l1.2 &lt;dbl&gt; 1, 2, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 2, 1, 1… ## $ l1.3 &lt;dbl&gt; 1, 1, 0, 2, 1, 1, 2, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1… ## $ l1.4 &lt;dbl&gt; 1, 2, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 1… ## $ l2.1 &lt;dbl&gt; 0, 2, 0, 2, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 2, 2, 2… ## $ l2.2 &lt;dbl&gt; 0, 1, 0, 2, 1, 2, 2, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1… ## $ l2.3 &lt;dbl&gt; 0, 2, 0, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1… ## $ l2.4 &lt;dbl&gt; 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1… ## $ l2.5 &lt;dbl&gt; 0, 2, 0, 2, 1, 1, 1, 2, 2, 0, 1, 1, 1, 0, 2, 1, 1… ## $ l3.1 &lt;dbl&gt; NA, 2, NA, 2, NA, 0, NA, 2, 1, NA, NA, NA, NA, NA… ## $ l3.2 &lt;dbl&gt; NA, 2, NA, 2, NA, 0, NA, 2, 1, NA, NA, NA, NA, NA… ## $ l3.3 &lt;dbl&gt; NA, 2, NA, 2, NA, 0, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l3.4 &lt;dbl&gt; NA, 2, NA, 2, NA, 0, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l3.5 &lt;dbl&gt; NA, 2, NA, 2, NA, 0, NA, 2, 1, NA, NA, NA, NA, NA… ## $ l4.1 &lt;dbl&gt; NA, 2, NA, 2, NA, 2, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l4.2 &lt;dbl&gt; NA, 2, NA, 2, NA, 1, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l4.3 &lt;dbl&gt; NA, 2, NA, 2, NA, 2, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l4.4 &lt;dbl&gt; NA, 2, NA, 2, NA, 2, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l4.5 &lt;dbl&gt; NA, 2, NA, 2, NA, 1, NA, 2, 2, NA, NA, NA, NA, NA… ## $ l4.6 &lt;dbl&gt; NA, 2, NA, 2, NA, 2, NA, 2, 1, NA, NA, NA, NA, NA… ## $ l5.1 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l5.2 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l5.3 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l5.4 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l5.5 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l6.1 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l6.2 &lt;dbl&gt; NA, 2, NA, 1, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l6.3 &lt;dbl&gt; NA, 2, NA, 1, NA, NA, NA, 1, NA, NA, NA, NA, NA, … ## $ l6.4 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ l6.5 &lt;dbl&gt; NA, 2, NA, 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, … ## $ n1 &lt;dbl&gt; 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2… ## $ n2 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2… ## $ n3 &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1… ## $ n4 &lt;dbl&gt; 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1… ## $ n5 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 2, 1… ## $ n6 &lt;dbl&gt; 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1… ## $ n7 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1… ## $ n8 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1… ## $ n9 &lt;dbl&gt; 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1… ## $ n10 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1… ## $ n11 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ n12 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2… ## $ n13 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ n14 &lt;dbl&gt; 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2… ## $ n15 &lt;dbl&gt; 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2… ## $ n16 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ n17 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2… ## $ n18 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2… ## $ n19 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2… ## $ n20 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ n21 &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2… ## $ n22 &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2… ## $ n23 &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2… ## $ n24 &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2… ## $ n25 &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2… ## $ n26 &lt;dbl&gt; 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2… ## $ n27 &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1… ## $ n28 &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 0, 1, 2, 2, 1… ## $ n29 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1… ## $ n30 &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1… ## $ n31 &lt;dbl&gt; NA, NA, NA, 1, NA, NA, 1, 1, 2, NA, NA, NA, NA, N… ## $ n32 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 1, 1, 1, 0, NA, NA, NA, N… ## $ n33 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 2, NA, 0, 1, NA, 1, N… ## $ n34 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 2, NA, 1, NA, NA, NA,… ## $ n35 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 1, 2, 2, 1, NA, NA, NA, N… ## $ n36 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 2, NA, 1, NA, NA, NA,… ## $ icar_assess_time &lt;dbl&gt; 0.7833333, 5.5166667, 1.7333333, 4.2166667, 0.616… ## $ ican_assess_time &lt;dbl&gt; 4.866667, 3.333333, 4.333333, 4.116667, 7.383333,… ## $ SubmissionDate_clean &lt;date&gt; 2025-09-23, 2025-09-23, 2025-09-23, 2025-09-23, … ## $ ass_lang_str &lt;chr&gt; &quot;Portuguese&quot;, &quot;Portuguese&quot;, &quot;Portuguese&quot;, &quot;Portug… ## $ ass_home_lang &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ enr_status &lt;chr&gt; &quot;Currently Enrolled&quot;, &quot;Currently Enrolled&quot;, &quot;Curr… ## $ HH_Weight_Provided &lt;dbl&gt; 874.2795, 874.2795, 874.2795, 874.2795, 874.2795,… ## $ MPL_math &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0… ## $ MPL_lang &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ MPL_both &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0… What does this tell us? Variable names (on the left) Variable types (&lt;chr&gt; for character/text, &lt;dbl&gt; for numeric) First few values for each variable 2.5 First Look at the Data # View first 5 rows head(data_weighted) ## # A tibble: 6 × 142 ## CountryName Tier_One_Unit VillageID Location HHID SubmissionDate duration ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Mozambique Niassa c101 Urban uuid:f58… 9/23/2025, 5:… 979 ## 2 Mozambique Niassa c101 Urban uuid:b98… 9/23/2025, 5:… 986 ## 3 Mozambique Niassa c101 Urban uuid:261… 9/23/2025, 5:… 2167 ## 4 Mozambique Niassa c101 Urban uuid:d46… 9/23/2025, 5:… 2078 ## 5 Mozambique Niassa c101 Urban uuid:df9… 9/23/2025, 5:… 1251 ## 6 Mozambique Niassa c101 Urban uuid:ade… 9/23/2025, 5:… 2199 ## # ℹ 135 more variables: hh06a &lt;dbl&gt;, hh06b &lt;dbl&gt;, hh06c &lt;dbl&gt;, hh06d &lt;dbl&gt;, ## # hh07a &lt;dbl&gt;, hh07b &lt;dbl&gt;, hh07c &lt;dbl&gt;, hh07d &lt;dbl&gt;, hh07e &lt;dbl&gt;, ## # hh07f &lt;dbl&gt;, hh07g &lt;dbl&gt;, hh07h &lt;dbl&gt;, hh07i &lt;dbl&gt;, hh07j &lt;dbl&gt;, ## # hh07k &lt;dbl&gt;, hh07l &lt;dbl&gt;, hh07m &lt;dbl&gt;, hh07n &lt;dbl&gt;, hh07o &lt;dbl&gt;, ## # hh07p &lt;dbl&gt;, hh07p_3 &lt;dbl&gt;, hh07q &lt;dbl&gt;, ChildID &lt;chr&gt;, ch02 &lt;dbl&gt;, ## # ch03 &lt;dbl&gt;, ch04a &lt;dbl&gt;, ch04b &lt;dbl&gt;, ch04c &lt;dbl&gt;, ch04d &lt;dbl&gt;, ## # ch04e &lt;dbl&gt;, ch04f &lt;dbl&gt;, ch05 &lt;dbl&gt;, ch06a &lt;dbl&gt;, ch06b &lt;dbl&gt;, … # View last 5 rows tail(data_weighted) ## # A tibble: 6 × 142 ## CountryName Tier_One_Unit VillageID Location HHID SubmissionDate duration ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Tanzania Tanga v472 Rural uuid:b07… 9/27/2025, 6:… 2470 ## 2 Tanzania Tanga v472 Rural uuid:fa7… 9/27/2025, 6:… 3105 ## 3 Tanzania Tanga v472 Rural uuid:fa7… 9/27/2025, 6:… 3105 ## 4 Tanzania Tanga v472 Rural uuid:fe8… 9/27/2025, 6:… 1896 ## 5 Tanzania Tanga v472 Rural uuid:ae8… 9/29/2025, 2:… 4161 ## 6 Tanzania Tanga v472 Rural uuid:ae8… 9/29/2025, 2:… 4161 ## # ℹ 135 more variables: hh06a &lt;dbl&gt;, hh06b &lt;dbl&gt;, hh06c &lt;dbl&gt;, hh06d &lt;dbl&gt;, ## # hh07a &lt;dbl&gt;, hh07b &lt;dbl&gt;, hh07c &lt;dbl&gt;, hh07d &lt;dbl&gt;, hh07e &lt;dbl&gt;, ## # hh07f &lt;dbl&gt;, hh07g &lt;dbl&gt;, hh07h &lt;dbl&gt;, hh07i &lt;dbl&gt;, hh07j &lt;dbl&gt;, ## # hh07k &lt;dbl&gt;, hh07l &lt;dbl&gt;, hh07m &lt;dbl&gt;, hh07n &lt;dbl&gt;, hh07o &lt;dbl&gt;, ## # hh07p &lt;dbl&gt;, hh07p_3 &lt;dbl&gt;, hh07q &lt;dbl&gt;, ChildID &lt;chr&gt;, ch02 &lt;dbl&gt;, ## # ch03 &lt;dbl&gt;, ch04a &lt;dbl&gt;, ch04b &lt;dbl&gt;, ch04c &lt;dbl&gt;, ch04d &lt;dbl&gt;, ## # ch04e &lt;dbl&gt;, ch04f &lt;dbl&gt;, ch05 &lt;dbl&gt;, ch06a &lt;dbl&gt;, ch06b &lt;dbl&gt;, … 2.6 Variable Names Exploration Let’s look at all variable names systematically: # List all variable names names(data_weighted) Together, let’s answer: Can you identify the design variables? (e.g., CountryName, VillageID, HHID, ChildID) Which variables are household-level? (Look for hh prefix - household characteristics) Which variables are child-level? (Look for ch prefix - child demographics, enrollment) Which variables are assessment outcomes? (Look for l for literacy, n for numeracy, MPL) Which are administrative/metadata? (e.g., SubmissionDate, duration) 2.7 Checking Variable Types # Look at variable types using glimpse (we used this earlier!) glimpse(data_weighted) # We can also look at specific variables more closely str(data_weighted$CountryName) # Character variable str(data_weighted$ch02) # Numeric variable (age) str(data_weighted$enr_status) # Character variable Let’s discuss together: Look at the output - what does &lt;chr&gt; mean? What about &lt;dbl&gt;? Can you spot which variables are character (text) vs numeric? Do the types make sense for what each variable represents? 2.8 Exercise 1.1: Exploring Specific Rows # YOUR TURN: View rows 10 to 20 of the dataset # Hint: Use slice() or bracket notation data_weighted[10:20, ] 2.9 Exercise 1.2: Exploring Variable Types # YOUR TURN: Look at the structure of these three variables: # - hh07a (household roofing material) # - ch03 (child&#39;s sex) # - MPL_math (math proficiency level) # Use str() to examine each one # What type is each variable? Does it make sense? str(data_weighted$hh07a) str(data_weighted$ch03) str(data_weighted$MPL_math) "],["part-2-understanding-your-countrys-geographic-structure.html", "Chapter 3 Part 2: Understanding Your Country’s Geographic Structure 3.1 Countries in the Dataset 3.2 Geographic Structure 3.3 Your Country’s Geographic Structure 3.4 Understanding Your Sample’s Hierarchy 3.5 Location Types 3.6 Exercise 2.1: Village-Level Exploration 3.7 Exercise 2.2: Regional Distribution", " Chapter 3 Part 2: Understanding Your Country’s Geographic Structure Core Question: How is the sample organized across regions and places in your country? 3.1 Countries in the Dataset Instructor Note: I’m demonstrating with the full multi-country dataset. Your output will show only your assigned country. # How many countries in the full dataset? unique(data_weighted$CountryName) ## [1] &quot;Mozambique&quot; &quot;Bangladesh&quot; &quot;Kenya&quot; &quot;Mali&quot; &quot;Mexico&quot; ## [6] &quot;Nicaragua&quot; &quot;Nepal&quot; &quot;Pakistan&quot; &quot;Tanzania&quot; &quot;Uganda&quot; ## [11] &quot;Senegal&quot; # Count of children per country data_weighted %&gt;% count(CountryName, name = &quot;n_children&quot;) %&gt;% arrange(desc(n_children)) ## # A tibble: 11 × 2 ## CountryName n_children ## &lt;chr&gt; &lt;int&gt; ## 1 Tanzania 13167 ## 2 Mali 9588 ## 3 Pakistan 9202 ## 4 Uganda 8883 ## 5 Mexico 8109 ## 6 Senegal 8098 ## 7 Mozambique 8022 ## 8 Kenya 6669 ## 9 Bangladesh 6479 ## 10 Nicaragua 6230 ## 11 Nepal 4694 For trainees: You should see only ONE country name when you run this code. That’s expected - you’re analyzing your country’s data. 3.2 Geographic Structure # Check the first-level administrative units data_weighted %&gt;% count(CountryName, Tier_One_Unit) %&gt;% arrange(CountryName, desc(n)) ## # A tibble: 267 × 3 ## CountryName Tier_One_Unit n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Bangladesh Dhaka 1343 ## 2 Bangladesh Chattogram 1108 ## 3 Bangladesh Khulna 913 ## 4 Bangladesh Rangpur 843 ## 5 Bangladesh Rajshahi 760 ## 6 Bangladesh Barishal 584 ## 7 Bangladesh Sylhet 469 ## 8 Bangladesh Mymensingh 459 ## 9 Kenya Nairobi City 218 ## 10 Kenya Kisii 213 ## # ℹ 257 more rows 3.3 Your Country’s Geographic Structure # What country are you working with? unique(data_weighted$CountryName) ## [1] &quot;Mozambique&quot; &quot;Bangladesh&quot; &quot;Kenya&quot; &quot;Mali&quot; &quot;Mexico&quot; ## [6] &quot;Nicaragua&quot; &quot;Nepal&quot; &quot;Pakistan&quot; &quot;Tanzania&quot; &quot;Uganda&quot; ## [11] &quot;Senegal&quot; # How many children in your country&#39;s sample? nrow(data_weighted) ## [1] 89141 # What administrative regions does your sample cover? data_weighted %&gt;% count(Tier_One_Unit, name = &quot;n_children&quot;) %&gt;% arrange(desc(n_children)) ## # A tibble: 267 × 2 ## Tier_One_Unit n_children ## &lt;chr&gt; &lt;int&gt; ## 1 Punjab 4119 ## 2 DAKAR 2238 ## 3 Noroeste 2091 ## 4 Sindh 1951 ## 5 DISTRICT DE BAMAKO 1742 ## 6 Khyber Pakhtunkhwa 1704 ## 7 SEGOU 1511 ## 8 Dhaka 1343 ## 9 Balochistan 1342 ## 10 BOUGOUNI 1267 ## # ℹ 257 more rows What to look for: - How many regions/provinces are represented? - Which region has the most children? - Is the distribution fairly balanced? 3.4 Understanding Your Sample’s Hierarchy Let’s explore how the sample is structured in YOUR country: # Summary of your country&#39;s sample structure data_weighted %&gt;% group_by(CountryName) %&gt;% # no need for this if you have only one country data summarise( country = unique(CountryName), n_regions = n_distinct(Tier_One_Unit), n_villages = n_distinct(VillageID), n_households = n_distinct(HHID), n_children = n(), children_per_hh = round(n() / n_distinct(HHID), 1), children_per_village = round(n() / n_distinct(VillageID), 1) ) ## # A tibble: 11 × 8 ## CountryName country n_regions n_villages n_households n_children ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Bangladesh Bangladesh 8 275 5493 6479 ## 2 Kenya Kenya 47 222 4337 6669 ## 3 Mali Mali 12 200 3829 9588 ## 4 Mexico Mexico 8 326 5329 8109 ## 5 Mozambique Mozambique 13 255 5064 8022 ## 6 Nepal Nepal 7 191 3795 4694 ## 7 Nicaragua Nicaragua 8 361 5756 6230 ## 8 Pakistan Pakistan 5 283 6053 9202 ## 9 Senegal Senegal 14 202 3974 8098 ## 10 Tanzania Tanzania 26 372 6880 13167 ## 11 Uganda Uganda 119 222 4267 8883 ## # ℹ 2 more variables: children_per_hh &lt;dbl&gt;, children_per_village &lt;dbl&gt; Let’s discuss: - How many villages were sampled in your country? - What’s the average number of children per household? - What’s the average number of children per village? - Does this match the sampling design (target of ~20 households per village)? 3.5 Location Types # Urban vs Rural distribution data_weighted %&gt;% count(CountryName, Location) %&gt;% pivot_wider(names_from = Location, values_from = n) ## # A tibble: 11 × 3 ## CountryName Rural Urban ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Bangladesh 4564 1915 ## 2 Kenya 3376 3293 ## 3 Mali 6922 2666 ## 4 Mexico 3266 4843 ## 5 Mozambique 4974 3048 ## 6 Nepal 1610 3084 ## 7 Nicaragua 2696 3534 ## 8 Pakistan 4865 4337 ## 9 Senegal 3219 4879 ## 10 Tanzania 7816 5351 ## 11 Uganda 6473 2410 3.6 Exercise 2.1: Village-Level Exploration # YOUR TURN: For YOUR country, find: # 1. Which village (VillageID) has the MOST children # 2. Which village has the FEWEST children # 3. Create a histogram showing distribution of children per village # Hint for #1 and #2: data_weighted %&gt;% group_by(VillageID) %&gt;% summarise(n_children = n()) %&gt;% arrange(desc(n_children)) # Hint for #3: # First create the summary, then pipe to ggplot() 3.7 Exercise 2.2: Regional Distribution # YOUR TURN: Create a summary for YOUR country showing: # - Number of children # - Number of unique households # - Number of villages # For each Tier_One_Unit × Location combination # This shows you which regions are urban vs rural # and how sample is distributed # Hint: Group by both Tier_One_Unit AND Location "],["part-3-basic-data-quality-checks.html", "Chapter 4 Part 3: Basic Data Quality Checks 4.1 Overall Missing Data Pattern 4.2 Examining Specific Missing Patterns 4.3 Using skim() for Quick Overview 4.4 Exploring Specific Variable Types 4.5 Numeric Variable Ranges 4.6 Exercise 3.1: Missing Data Investigation 4.7 Exercise 3.2: Exploring Specific Variable Types", " Chapter 4 Part 3: Basic Data Quality Checks Core Question: Where might analysis become tricky later? 4.1 Overall Missing Data Pattern # What percentage of data is missing overall? round(mean(is.na(data_weighted)) * 100, 2) ## [1] 15.92 # Which variables have the MOST missing values? missing_summary &lt;- data_weighted %&gt;% summarise(across(everything(), ~sum(is.na(.)))) %&gt;% pivot_longer(everything(), names_to = &quot;variable&quot;, values_to = &quot;n_missing&quot;) %&gt;% mutate(pct_missing = round(n_missing / nrow(data_weighted) * 100, 1)) %&gt;% filter(n_missing &gt; 0) %&gt;% arrange(desc(n_missing)) head(missing_summary, 15) ## # A tibble: 15 × 3 ## variable n_missing pct_missing ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 ch07b 86866 97.4 ## 2 ch07c 86866 97.4 ## 3 ch07d 86866 97.4 ## 4 ch07a 79232 88.9 ## 5 hh07p_3 76035 85.3 ## 6 pt01f 58492 65.6 ## 7 n33 55500 62.3 ## 8 pt02d 52838 59.3 ## 9 n34 52545 58.9 ## 10 n36 52545 58.9 ## 11 ch10b 49905 56 ## 12 n32 46860 52.6 ## 13 n35 46860 52.6 ## 14 pt02f 46475 52.1 ## 15 pt01d 45286 50.8 What to notice: Some variables have a lot of missing data (&gt;50%) Why might literacy level 6 (l6.) have more missing than level 1 (l1.)? Some household variables (hh07*) also have missing values This is expected - we’ll explore patterns in the next session 4.2 Examining Specific Missing Patterns # Compare missing rates between literacy levels literacy_missing &lt;- data_weighted %&gt;% summarise( L1_missing = sum(is.na(l1.1)) / n() * 100, L2_missing = sum(is.na(l2.1)) / n() * 100, L3_missing = sum(is.na(l3.1)) / n() * 100, L4_missing = sum(is.na(l4.1)) / n() * 100, L5_missing = sum(is.na(l5.1)) / n() * 100, L6_missing = sum(is.na(l6.1)) / n() * 100 ) # Missing data by literacy level literacy_missing ## # A tibble: 1 × 6 ## L1_missing L2_missing L3_missing L4_missing L5_missing L6_missing ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00337 0.00337 29.2 29.2 42.6 50.1 Insight: This pattern tells us something about the assessment structure - children who couldn’t answer earlier levels weren’t given harder questions! 4.3 Using skim() for Quick Overview # Quick comprehensive summary skim(data_weighted %&gt;% select(1:10)) # First 10 variables only Table 4.1: Data summary Name data_weighted %&gt;% select(… Number of rows 89141 Number of columns 10 _______________________ Column type frequency: character 6 numeric 4 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace CountryName 0 1 4 10 0 11 0 Tier_One_Unit 0 1 3 27 0 267 0 VillageID 0 1 4 4 0 627 0 Location 0 1 5 5 0 2 0 HHID 0 1 41 41 0 54777 0 SubmissionDate 0 1 20 23 0 33348 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist duration 0 1.00 2188.50 1717.59 -38130 1028 1704 2831 20142 ▁▁▁▇▁ hh06a 0 1.00 134981.57 10296179.81 1 4 6 8 877837912 ▇▁▁▁▁ hh06b 0 1.00 1.01 0.91 0 0 1 1 7 ▇▂▁▁▁ hh06c 40598 0.54 1.26 0.44 1 1 1 2 2 ▇▁▁▁▃ 4.4 Exploring Specific Variable Types Let’s look at categorical variables together: # Example: Enrollment status - a key outcome variable data_weighted %&gt;% count(enr_status) %&gt;% mutate( percent = round(n/sum(n)*100, 1), description = paste0(n, &quot; (&quot;, percent, &quot;%)&quot;) ) ## # A tibble: 3 × 4 ## enr_status n percent description ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Currently Enrolled 79397 89.1 79397 (89.1%) ## 2 Never Enrolled 7517 8.4 7517 (8.4%) ## 3 Out of School 2227 2.5 2227 (2.5%) # Example: Child&#39;s sex (ch03) data_weighted %&gt;% count(ch03) %&gt;% mutate(percent = round(n/sum(n)*100, 1)) ## # A tibble: 2 × 3 ## ch03 n percent ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 45213 50.7 ## 2 2 43928 49.3 Together let’s answer: What are the unique values in enrollment status? What percentage of children are currently enrolled? Is the sex distribution balanced (roughly 50/50)? Are there any unexpected categories or values? # Example: Looking at a binary (0/1) variable # hh07e: Does household have electricity? summary(data_weighted$hh07e) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 1.000 1.000 0.987 1.000 1.000 31718 table(data_weighted$hh07e, useNA = &quot;ifany&quot;) ## ## 0 1 &lt;NA&gt; ## 770 56653 31718 Note: Even though this is coded as 0/1 (numeric), it’s really a categorical variable (No/Yes). 4.5 Numeric Variable Ranges Let’s look at some numeric variables to check for impossible or suspicious values: # Example: Duration variable (interview duration in seconds) summary(data_weighted$duration) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -38130 1028 1704 2188 2831 20142 # Convert to minutes for easier interpretation summary(data_weighted$duration / 60) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -635.50 17.13 28.40 36.47 47.18 335.70 # Check for impossible values # How many interviews have negative duration? sum(data_weighted$duration &lt; 0, na.rm = TRUE) ## [1] 8 # How many interviews lasted over 5 hours (300 minutes)? sum(data_weighted$duration &gt; 18000, na.rm = TRUE) ## [1] 11 Red flags to discuss: Negative duration values are impossible - likely data entry errors Very long durations might indicate the device was left on These will need to be addressed in data cleaning # Example: Household size (hh06a) summary(data_weighted$hh06a) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 4 6 134982 8 877837912 # That maximum value looks suspicious! cat(&quot;\\nHouseholds with more than 20 members:\\n&quot;) ## ## Households with more than 20 members: data_weighted %&gt;% filter(hh06a &gt; 20) %&gt;% count(hh06a) %&gt;% arrange(desc(hh06a)) %&gt;% head(10) ## # A tibble: 10 × 2 ## hh06a n ## &lt;dbl&gt; &lt;int&gt; ## 1 877837912 1 ## 2 877532724 1 ## 3 876922837 1 ## 4 866819197 2 ## 5 861792768 2 ## 6 853062368 1 ## 7 843555304 1 ## 8 785593874 1 ## 9 783035698 1 ## 10 764721593 2 Discussion: - A household with 877 million members is clearly an error! - Even 100+ members is suspicious - What might cause these errors? How should we handle them? 4.6 Exercise 3.1: Missing Data Investigation # YOUR TURN: Investigate missing data in parent variables (pt prefix) # 1. Count missing values in pt01b (mother&#39;s age) # 2. Count missing values in pt02b (father&#39;s age) # 3. Calculate the percentage missing for each # 4. Why might parent data be missing? # Hint: Use sum(is.na(variable_name)) to count missing # Hint: Use sum(is.na(variable_name)) / nrow(data_weighted) * 100 for percentage 4.7 Exercise 3.2: Exploring Specific Variable Types # YOUR TURN: Use skim() to create separate summaries for: # 1. Child-level variables (start with &#39;ch&#39;) # 2. Household-level variables (start with &#39;hh&#39;) # Hint: Use select(starts_with(&quot;ch&quot;)) to select by prefix # BONUS: Which has more missing values - household or child variables? "],["part-4-creating-basic-summaries.html", "Chapter 5 Part 4: Creating Basic Summaries 5.1 Summary Table: Sample Size by Country 5.2 Visualizing Geographic Distribution 5.3 Creating a Variable Inventory 5.4 Exercise 4.1: Urban-Rural Distribution 5.5 Exercise 4.2: Create Your Own Visualization", " Chapter 5 Part 4: Creating Basic Summaries Core Question: What does the sample look like in simple terms? These summaries help us understand who is represented in the data. They are not population estimates — for that, we would need to use the survey design object with weighted functions. 5.1 Summary Table: Sample Size by Country Instructor Note: This shows all countries. Trainees should adapt the code to summarize by Tier_One_Unit for their country. sample_summary &lt;- data_weighted %&gt;% group_by(CountryName) %&gt;% summarise( N_children = n(), N_households = n_distinct(HHID), N_villages = n_distinct(VillageID), N_admin_units = n_distinct(Tier_One_Unit), Children_per_HH = round(n() / n_distinct(HHID), 1) ) sample_summary ## # A tibble: 11 × 6 ## CountryName N_children N_households N_villages N_admin_units Children_per_HH ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Bangladesh 6479 5493 275 8 1.2 ## 2 Kenya 6669 4337 222 47 1.5 ## 3 Mali 9588 3829 200 12 2.5 ## 4 Mexico 8109 5329 326 8 1.5 ## 5 Mozambique 8022 5064 255 13 1.6 ## 6 Nepal 4694 3795 191 7 1.2 ## 7 Nicaragua 6230 5756 361 8 1.1 ## 8 Pakistan 9202 6053 283 5 1.5 ## 9 Senegal 8098 3974 202 14 2 ## 10 Tanzania 13167 6880 372 26 1.9 ## 11 Uganda 8883 4267 222 119 2.1 Note: These are sample counts (unweighted), not population estimates. For Trainees (Single-Country Data): Adapt the code above by replacing CountryName with Tier_One_Unit to see your sample distribution across administrative regions: # Your single-country version data_weighted %&gt;% group_by(Tier_One_Unit) %&gt;% summarise( N_children = n(), N_households = n_distinct(HHID), N_villages = n_distinct(VillageID), Children_per_HH = round(n() / n_distinct(HHID), 1) ) %&gt;% arrange(desc(N_children)) 5.2 Visualizing Geographic Distribution ggplot(sample_summary, aes(x = reorder(CountryName, N_children), y = N_children)) + geom_col(fill = &quot;#0B8A8F&quot;) + coord_flip() + labs(title = &quot;Child Sample Size by Country&quot;, subtitle = &quot;Number of children aged 5-16 years assessed&quot;, x = &quot;Country&quot;, y = &quot;Number of Children&quot;) + theme_minimal() For Trainees (Single-Country Data): Create a similar visualization for your administrative regions: # Your single-country visualization data_weighted %&gt;% group_by(Tier_One_Unit) %&gt;% summarise(N_children = n()) %&gt;% ggplot(aes(x = reorder(Tier_One_Unit, N_children), y = N_children)) + geom_col(fill = &quot;#0B8A8F&quot;) + coord_flip() + labs(title = &quot;Child Sample Size by Administrative Region&quot;, subtitle = paste(&quot;Number of children in&quot;, unique(data_weighted$CountryName)), x = &quot;Administrative Region&quot;, y = &quot;Number of Children&quot;) + theme_minimal() 5.3 Creating a Variable Inventory Let’s create a useful reference by grouping variables into categories: # First, let&#39;s just look at all variable names grouped logically # We&#39;ll identify patterns by looking at prefixes # Design/Admin variables design_vars &lt;- c(&quot;CountryName&quot;, &quot;Tier_One_Unit&quot;, &quot;VillageID&quot;, &quot;Location&quot;, &quot;HHID&quot;, &quot;ChildID&quot;, &quot;HH_Weight_Provided&quot;, &quot;SubmissionDate&quot;, &quot;duration&quot;) # Count variables by prefix pattern category_counts &lt;- tibble( category = c(&quot;Design/Admin&quot;, &quot;Household (hh)&quot;, &quot;Child (ch)&quot;, &quot;Parent (pt)&quot;, &quot;Literacy (l)&quot;, &quot;Numeracy (n)&quot;, &quot;Outcomes (MPL)&quot;), count = c( length(design_vars), sum(str_starts(names(data_weighted), &quot;hh&quot;)), sum(str_starts(names(data_weighted), &quot;ch&quot;)), sum(str_starts(names(data_weighted), &quot;pt&quot;)), sum(str_starts(names(data_weighted), &quot;l[0-9]&quot;)), sum(str_starts(names(data_weighted), &quot;n[0-9]&quot;)), sum(str_starts(names(data_weighted), &quot;MPL&quot;)) ) ) category_counts ## # A tibble: 7 × 2 ## category count ## &lt;chr&gt; &lt;int&gt; ## 1 Design/Admin 9 ## 2 Household (hh) 22 ## 3 Child (ch) 23 ## 4 Parent (pt) 11 ## 5 Literacy (l) 30 ## 6 Numeracy (n) 36 ## 7 Outcomes (MPL) 3 # Look at examples from each category examples_by_category &lt;- list( household = names(data_weighted)[str_starts(names(data_weighted), &quot;hh&quot;)][1:5], child = names(data_weighted)[str_starts(names(data_weighted), &quot;ch&quot;)][1:5], literacy = names(data_weighted)[str_starts(names(data_weighted), &quot;l[0-9]&quot;)][1:5] ) examples_by_category ## $household ## [1] &quot;hh06a&quot; &quot;hh06b&quot; &quot;hh06c&quot; &quot;hh06d&quot; &quot;hh07a&quot; ## ## $child ## [1] &quot;ch02&quot; &quot;ch03&quot; &quot;ch04a&quot; &quot;ch04b&quot; &quot;ch04c&quot; ## ## $literacy ## [1] &quot;l1.1&quot; &quot;l1.2&quot; &quot;l1.3&quot; &quot;l1.4&quot; &quot;l2.1&quot; Discussion: - How are variables organized in this dataset? - Which categories have the most variables? - What does this tell us about the survey’s focus? - Why do we have so many numeracy items (n1-n36)? 5.4 Exercise 4.1: Urban-Rural Distribution # YOUR TURN: Create a summary table showing: # - Number of children by Location (urban/rural) # - Number of households by Location # For YOUR country (single row) or by Country (if using multi-country data) # Hint: Use group_by(Location) and summarise(), then pivot_wider() # BONUS: Also break it down by Tier_One_Unit AND Location 5.5 Exercise 4.2: Create Your Own Visualization # YOUR TURN: Create a bar chart appropriate for YOUR data: # # If you have SINGLE COUNTRY data: # - Show number of children by Tier_One_Unit (administrative regions) # - Use fill = Location to distinguish urban vs rural # # If you have MULTI-COUNTRY data: # - Show number of children by Country # - Use fill = Location to distinguish urban vs rural # # Hint: Use geom_col() with fill aesthetic "],["summary-and-key-takeaways.html", "Chapter 6 Summary and Key Takeaways 6.1 What We Accomplished Today 6.2 Key Distinctions We Made Today 6.3 Next Steps 6.4 Best Practices Learned", " Chapter 6 Summary and Key Takeaways 6.1 What We Accomplished Today At this point, we have: ✅ Verified the dataset structure — understood dimensions and unit of analysis ✅ Understood the geographic hierarchy — how children nest within households, villages, and regions (and countries for multi-country data) ✅ Connected structure to survey design — seen how weights vary and why they matter ✅ Identified data quality issues — mapped where missing values exist ✅ Created descriptive summaries — understood sample composition (not population estimates) We are now ready to explore missing values and variable distributions in detail. 6.2 Key Distinctions We Made Today What We Did What We Did NOT Do Explored data structure Make population inferences Identified missing values Decide how to handle missingness Described the sample Estimate population parameters Connected to survey design Re-create the design object Found data quality issues Fix or clean the data 6.3 Next Steps In the next session, we will: Explore missing values in detail — patterns, mechanisms, implications Understand distributions — shape, outliers, transformations Begin data cleaning — addressing issues we identified today Create derived variables — preparing variables for analysis 6.4 Best Practices Learned ✅ Always start by understanding your data structure ✅ Verify design variables before creating survey object ✅ Check for missing values and data quality issues early ✅ Create systematic summaries to document your data ✅ Test your survey design object before full analysis "],["additional-resources.html", "Chapter 7 Additional Resources 7.1 Useful Functions Reference 7.2 For Practice", " Chapter 7 Additional Resources 7.1 Useful Functions Reference Task Function Example Data dimensions dim() dim(data_weighted) Variable names names() names(data_weighted) Data structure glimpse() glimpse(data_weighted) Quick summary skim() skim(data_weighted) Missing values is.na() sum(is.na(var)) Unique values n_distinct() n_distinct(var) 7.2 For Practice Try exploring these on your own: Create a complete variable inventory for ALL variables Investigate patterns of missing data across regions (or countries if using multi-country data) Create visualizations for the geographic distribution in your context Calculate the average number of households per village by region (or by country) Remember: Getting to know your data is not a one-time task. You’ll continually discover new aspects of your data throughout the analysis process! "],["logistic-regression.html", "Chapter 8 Logistic Regression 8.1 What logistic regression models 8.2 Simple versus multiple logistic regression 8.3 Data 8.4 Simple Logistic Regression 8.5 Multiple binary logistic regression 8.6 Interaction 8.7 Reporting Results", " Chapter 8 Logistic Regression Many questions in early-foundational learning research ask about the likelihood of a binary outcome - something that is either “yes/no” or “0/1”. For instance, how likely is a child to meet the minimum proficiency level in both maths and reading, given the child’s age and whether the language of assessment matches with the child’s home language. In this lesson, we use the PAL Network’s ICAN-ICAR 2025 survey data to model binary outcomes using logistic regression. A simple linear regression can be used as a “linear probability model”, but it often produces predicted values below 0 or above 1 and does not handle the changing variability that comes with binary outcomes. Logistic regression avoids these issues by modeling the log-odds of the outcome, which naturally maps to predicted probabilities between 0 and 1. Because ICAN/ICAR is collected using a complex survey design (stratification, clustering, and survey weights), we fit models using the survey package: we first define the design with svydesign() and then estimate logistic regression with svyglm() so that standard errors and inference reflect the sampling design. 8.1 What logistic regression models Suppose \\(Y\\) indicates whether a learner meets the MPL in maths, where \\(Y = 1\\) means “meets MPL” and \\(Y = 0\\) means “does not meet MPL”. Let \\(X_1\\) be ICAN assessment time for maths. Figure 8.1: Linear regression of a binary Y variable (0 = learner do not meet MPL for Maths, 1 = learner meets MPL for Maths) on a measurement X variable (child’s age in years). A linear regression of a binary outcome is sometimes called a linear probability model. It can produce predicted values outside the \\([0,1]\\) range. For example in @fig-simplereg, \\[\\hat{Y} = -0.351 + 0.0859X_1,\\]predictions exceed 1 when \\(X_1 &gt; 15.8\\). This is not meaningful because probabilities must lie between 0 and 1. In addition, with a binary outcome the variability changes with the mean (because \\(Var(Y |X) = p(1- p)\\)), so the constant-variance assumption behind ordinary linear regression is not appropriate here. Logistic regression starts by modeling the probability of “success”: \\[ p = P(Y = 1 | X). \\] Since \\(Y\\) is binary, \\(P(Y = 0 | X) = 1-p\\). The odds of meeting MPL are defined as \\[\\text{odds} = \\frac{p}{1 - p}.\\] Odds range from 0 to \\(\\infty\\). For example, if \\(p = 0.2\\), then the odds are \\(0.2/0.8 = 0.25\\). You can say odds are 0.25 to 1,or 1 to 4. Logistic regression assumes the log-odds (the logit) are linearly related with the predictors: \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 \\] Solving for \\(p\\) gives the logistic curve: \\[ p = \\frac{\\text{exp}(\\beta_0 + \\beta_1X_1)}{1 + \\text{exp}(\\beta_0 + \\beta_1X_1)}, \\] which always stays between 0 and 1. Here, \\(\\beta_1\\) is the change in the log-odds per one-unit increase in \\(X_1\\), and \\(\\text{exp}(\\beta_1)\\) is the odds ratio associated with a one-unit increase in \\(X_1\\). If the logit is a linearly related to the \\(X\\) variables, then the probability \\(p\\) is a non-linear, S-shaped function with respect to the \\(X\\) variables as in @fig-logreg. Figure 8.2: Predicted probability as a logit function X Logistic regression is widely used in many fields whenever we want to predict the probability of a binary outcome (yes/no, 0/1). For example: Medicine: to estimate the probability that a patient has (or will develop) a disease using medical history and test results. Banking/Finance: to estimate the probability that a borrower will default, based on income, credit history, and other profile information. Politics: to estimate the probability that a citizen will vote for a particular party or candidate. Many others: marketing (purchase vs not), education (pass vs fail), and program evaluation (participate vs not). In this lesson, we apply logistic regression to ICAN/ICAR survey data to model whether a learner meets the Minimum Proficiency Level (MPL). 8.2 Simple versus multiple logistic regression Binary logistic regression has an outcome \\(Y\\) which has two categories (e.g., \\(Y =1\\), \\(Y = 0\\)). We distinguish between two common forms: Simple logistic regression uses one predictor: \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0+\\beta_1X_1 \\] Multiple logistic regression uses two or more predictors: \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\ldots+ \\beta_kX_k \\] Multiple logistic regression is often more useful because it lets us adjust for other factors. Each coefficient describes the relationship between a predictor and the outcome holding the other predictors constant. Because the ICAN/ICAR data come from a complex survey design, we estimate both simple and multiple logistic regression models using svmglm() with a design object created by svydesign(). 8.3 Data We create an analysis-ready data by, (i) restricting to a specific country and learner subgroup, (ii) selecting a small set of variables needed for the models, and (iii) re-coding key variables into analysis-friendly formats (especially binary predictors and the MPL outcome). ch02: Child’s age ch03: Child’s sex ch09: Has the child brought any material to read (other than school textbooks) from the school library MPL_both: Whether the learner meets minimum proficiency level in both maths and reading CountryName: Unique country name Tier_One_Unit: Geographical divisions within one country (county, state, province, etc) VillageID: Unique village identifiers HHID: Unique household identifier 8.3.1 Import, subset, and keep only needed variables # load packages library(tidyverse) library(survey) library(skimr) # load the data dat = read_csv(&quot;data/2025_PAL_ICAN-ICAR_data.csv&quot;) |&gt; filter( CountryName == &quot;Senegal&quot; &amp; # restrict to this country enr_status == &quot;Currently Enrolled&quot; # currently enrolled in school ) |&gt; select( Location, CountryName, Tier_One_Unit, VillageID, HHID, HH_Weight_Provided, ch02, ch03, ch09, MPL_both ) |&gt; rename( age = ch02, sex = ch03, books = ch09, mpl_both = MPL_both ) 8.3.2 Recode variables (labels + reference levels) dat = dat |&gt; mutate( # Outcome (0/1 -&gt; No/Yes) mpl_both = factor(mpl_both, levels = c(0,1), labels = c(&quot;No&quot;, &quot;Yes&quot;)), # Binary predictors (0/1 -&gt; No/Yes) books = factor(books, levels = c(0,1), labels = c(&quot;No&quot;, &quot;Yes&quot;)), sex = factor(sex, levels = c(1,2), labels = c(&quot;Female&quot;, &quot;Male&quot;)), Location = factor(Location) ) Always recognize the order of the levels for the dependent variable because it will have an impact on the interpretations. In R, the first level from levels() is always taken as the reference level. In our case, the first level is not meeting the MPL and the second level is meeting the MPL for both maths and reading. levels(dat$mpl_both) ## [1] &quot;No&quot; &quot;Yes&quot; This means that when we estimate the impact of the independent variable(s) it will be on the learner meeting the MPL (and not on the learner not meeting the MPL). Here is the preview of the data and some descriptive statistics. dat |&gt; skim() Table 8.1: Data summary Name dat Number of rows 6429 Number of columns 10 _______________________ Column type frequency: character 4 factor 4 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace CountryName 0 1 7 7 0 1 0 Tier_One_Unit 0 1 5 11 0 14 0 VillageID 0 1 4 4 0 201 0 HHID 0 1 41 41 0 3399 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Location 0 1 FALSE 2 Urb: 4021, Rur: 2408 sex 0 1 FALSE 2 Fem: 3765, Mal: 2664 books 0 1 FALSE 2 No: 5285, Yes: 1144 mpl_both 0 1 FALSE 2 No: 4755, Yes: 1674 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist HH_Weight_Provided 0 1 504.76 171.05 183.02 399.55 508.64 535.41 1262.94 ▃▇▁▁▁ age 0 1 10.58 3.11 5.00 8.00 11.00 13.00 16.00 ▇▇▇▇▇ 8.3.3 Why we need survey design information The ICAN/ICAR data come from a complex, multi-stage sample (clustered and stratified). To get correct standard errors, we create a survey design object that tells R which variables represent strata, primary sampling units (PSUs), and sampling weights. After this step, we fit models using svyglm() with the design object rather than using glm() directly. options( survey.lonely.psu = &quot;adjust&quot;, survey.adjust.domain.lonely = TRUE ) des = svydesign( ids = ~interaction(CountryName, VillageID) + HHID, strata = ~interaction(CountryName, Tier_One_Unit), weights= ~HH_Weight_Provided, nest = TRUE, data = dat ) 8.4 Simple Logistic Regression A simple logistic regression models a binary outcome using one predictor. We will illustrate simple logistic regression with: a continuous predictor, and a categorical predictor. 8.4.1 Continuous predictor: child’s age Suppose we want to model whether a learner meets MPL for both maths and reading (MPL_both) using the the child’s age (age). Here. m1 = svyglm(mpl_both ~ age, family = quasibinomial, design =des, na.action = na.omit ) m1 |&gt; tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5.50 0.184 -29.9 5.37e-73 ## 2 age 0.391 0.0145 26.9 5.50e-66 8.4.1.1 Interpreting the coefficient (log-odds) In a logistic model, \\[ \\text{log}(\\frac{p}{1-p}) = \\beta_0+\\beta_1X \\] so \\(\\beta_1\\) is the change in log-odds for a one-unit increase in \\(X\\) (here, one more year in child’s age). If \\(\\beta_1 &gt; 0\\): odds (and probability) of meeting MPL for both tends to increase as \\(X\\) increases. If \\(\\beta_1 &lt; 0\\): odds (and probability) of meeting MPL for both tend to decrease as \\(X\\) increases. Here, \\(\\hat{\\beta_1} \\approx 0.39\\), meaning that one year increase in child’s age leads to \\(0.39\\) increase in the log odds of a child meeting MPL for both maths and reading. Older children are more likely in odds terms to meet MPL for both maths and reading, holding the model as specified. 8.4.1.2 Interpreting the odds ratio Exponentiation of the \\(\\hat{\\beta_1}\\) coefficient converts back to the odds ratio of a child meeting MPL for both \\((Y = 1)\\). exp(coef(m1)[&quot;age&quot;]) ## age ## 1.478525 We can say that an additional year in the child’s age multiplies odds of meeting MPL by \\(~1.48\\). This means that the odds of meeting MPL for both maths and reading are about \\(48\\%\\) higher per additional year in the child’s age (because \\(1.4785 - 1 \\approx 0.4785\\)). Also, R performs a hypothesis test for each coefficient, that is \\(H_0: \\beta_j = 0\\) versus \\(H_0: \\beta_j \\neq 0\\) for \\(j = [0, 1]\\) via the Wald test, and prints the p-values in the last column. We can thus compare these p-values to the chosen significance level (usually \\(\\alpha = 0.05\\)) to conclude whether or not each of the coefficient is significantly different from zero. The lower the p-value, the more evidence that the coefficient is different from zero. This is similar to the linear regression. Here, the p-value \\(&lt; 0.05\\). This means that there is sufficient evidence that the age coefficient is statistically different from zero at the \\(5\\%\\) significance level. 8.4.1.3 Predicted probabilities A good next step is to use the model to compute the predicted probabilities for few ages. Suppose we would like to predict the probability of meeting MPL for both maths and reading for a child that is aged \\(7, 10, 13, 16\\): newdat = data.frame(age = c(7, 10, 13, 16)) # get predictions on the link (logit) scale pred = predict(m1, newdata = newdat) newdat$probabilities = plogis(pred) #convert logit --&gt; probability newdat ## age probabilities ## 1 7 0.05960664 ## 2 10 0.17003253 ## 3 13 0.39836984 ## 4 16 0.68154314 As the child grows in years the likelihood of them meeting the MPL for both increases substantially. We can also visualize the results of our model below: library(sjPlot) # plot plot_model( m1, type = &quot;pred&quot;, terms= &quot;age&quot; ) + labs(y = &quot;Probability of MPL for both maths &amp; reading&quot;, x = &quot;Child&#39;s Age in Years&quot;, title = &quot;Predicted Probabilities of MPL for both Maths &amp; Reading using Age&quot;) + theme_classic() The above plot shows that probability of a child meeting MPL for both reading and maths increases with the child’s age. 8.4.2 Categorical predictor example: Books Suppose we are now interested in predicting the probability of a child meeting MPL for both using whether the child has brought home any material (other than textbooks) to read from the school library (books). Recall that when the predictor was continuous, \\(e^{\\hat{\\beta_1}}\\) was the multiplicative change in the odds in favor of \\(Y = 1\\) as \\(X_1\\) increases by one unit. With \\(X_1\\) being books, a categorical variable, the only increase possible is 0 to 1 (or from 1 to 2 if sex is encoded as a factor). So, we can write the interpretation in terms of Yes/No: \\(e^{\\hat{\\beta_1}}\\) is the multiplicative change of the odds in favor of \\(Y = 1\\) as No becomes Yes, keeping in mind the order of the level for the variable books. # levels for books variable levels(dat$books) ## [1] &quot;No&quot; &quot;Yes&quot; So, it is indeed the multiplicative change of the odds in favor of \\(Y = 1\\) as No becomes Yes. If the level No came before the level Yes in the data, it would have been the opposite. We fit the model again: m2 = svyglm(mpl_both ~ books, family = quasibinomial, design =des, na.action = na.omit ) m2 |&gt; tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.14 0.0635 -17.9 2.18e-42 ## 2 booksYes 0.492 0.0941 5.23 4.60e- 7 8.4.2.1 Interpreting the coefficient (log-odds) As the No becomes Yes, the log-odds of meeting MPL for both changes by \\(0.492\\). The odds (and probabilities) tend to increase as the child start to bring back materials to read from the school library. 8.4.2.2 Interpreting the odds ratio Odds ratio for books variable are exp(coef(m2)[&quot;booksYes&quot;]) ## booksYes ## 1.635037 For children that bring home study materials from the school library, their odds of meeting MPL is multiplied by a factor of \\(e^{0.492} = 1.635\\) relative to those who do not bring back home study materials from the school library. This means that the odds of meeting MPL for both reading and maths are \\((1.635 -1)*100 = 63.5\\%\\) higher for those children who bring home study materials from the school library than those who do not bring back home study materials. The interpretation of the intercept \\(\\hat{\\beta_0} = -1.138\\) gives the probability of a child meeting MPL when \\(X= 0\\). In our case, this simply maps to those children who do not bring back study materials from the school library. Therefore, the probability of a a child who does not bring back home study material meeting MPL for both reading and math is: exp(coef(m2)[1]) / (1+exp(coef(m2)[1])) ## (Intercept) ## 0.2426628 The model’s intercept tells us that the probability of a child that does not bring back home study materials from the school library meeting MPL for both is only \\(24,2\\%\\). 8.4.2.3 Predicted probabilities As with the continuous predictor, predictions can also be made with the predict() function. Suppose we want to predict the probability of a child who brings back home study material from the school library meeting MPL for both reading and math: # predict probability to meet MPL newdat = data.frame(books = c(&quot;Yes&quot;, &quot;No&quot;)) pred = predict(m2, newdata = newdat, type = &quot;link&quot; ) newdat$probabilities = plogis(pred) newdat ## books probabilities ## 1 Yes 0.3437854 ## 2 No 0.2426628 Based on this model, it is predicted that a child who brings back home study material from the school library has a \\(34,4\\%\\) chance of meeting MPL for both reading and math. This is higher relative to those children who do not bring back home study materials. We can also visualize these results using the plot_model function: # plot plot_model(m2, type = &quot;pred&quot;, terms = &quot;books&quot; ) + labs(y = &quot;Probability of MPL for both maths &amp; reading&quot;, title = &quot;Predicted Probabilities of MPL for both Maths &amp; Reading using Books&quot; ) + theme_classic() The points correspond to the predicted probabilities, and the bars correspond to their confidence intervals. 8.5 Multiple binary logistic regression The interpretation of the coefficients in multiple logistic regression is similar to that of simple logistic regression, except that this time it estimates the multiplicative change in the log-odds in favor of \\(Y = 1\\) when \\(X\\) increases by one unit, while holding other predictors constant. For this illustration, suppose we want to predict the MPL for both reading and math using age. sex, and books. # fit the model m3 = svyglm(mpl_both ~ age + sex + books, family = quasibinomial, design =des, na.action = na.omit ) # print results m3 |&gt; tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5.53 0.181 -30.6 4.76e-74 ## 2 age 0.389 0.0145 26.9 1.41e-65 ## 3 sexMale -0.0455 0.0705 -0.646 5.19e- 1 ## 4 booksYes 0.393 0.105 3.74 2.46e- 4 Based on the p-values, we can conclude that, at the \\(5\\%\\) significance level, only age and books are significantly associated with MPL for both reading and math (p-value \\(&lt; 0.05\\)). Similar to simple logistic regression, it is easier to interpret these relationships through the odds ratios. But this time, we also print \\(95\\%\\) CI of the odds ratios in addition to the OR (rounded to 3 decimals) so that we can easily see which ones are significantly different from 1. If \\(95\\%\\) CI include the value of 1 in their bound, then there is no significant association between the MPL both and the predictor in question. This is another way of confirming the significance of a predictor. # OR and 95% CI round(exp(cbind(OR = coef(m3), confint(m3))), 3) ## OR 2.5 % 97.5 % ## (Intercept) 0.004 0.003 0.006 ## age 1.476 1.434 1.519 ## sexMale 0.956 0.831 1.098 ## booksYes 1.482 1.204 1.824 From the OR and their \\(95\\%\\) CI computed above, we can conclude that: age: an additional year in the child’s age multiplies odds of meeting MPL by \\(~1.48\\).This means that the odds of meeting MPL for both maths and reading are about \\(48\\%\\) higher per additional year in the child’s age, ceteris paribus. books: the odds of a child that brings back home study material from the school library meeting MPL for both reading and math are \\(1.482\\) times higher than the child who does not bring back home study material, ceteris paribus. We refrain from interpreting the sex as it not significant at the \\(5\\%\\) significance level (also, 1 is included in its \\(95\\%\\) CI). For illustrative purposes, suppose now we want to predict the probability that a new child meets MPL for both reading and math. Suppose that this child is from Senegal, aged 15 years old, male, and does not bring back home study material from the school library: # create data frame of new learner new_learner = data.frame( age = 15, sex = &quot;Male&quot;, books = &quot;No&quot; ) # predict probability to meet MPL both pred = predict(m3, newdata = new_learner, type = &quot;link&quot; ) # print results new_learner$probabilities = plogis(pred) new_learner ## age sex books probabilities ## 1 15 Male No 0.5646007 If we trust our logistic regression model, the probability that this new child meets MPL for both reading and math is predicted to be \\(56.5\\%\\). We can also visualize the effect of age and books on the predicted probability of meeting MPL in @fig-vistwo. plot_model(m3, type = &quot;pred&quot;, terms = c(&quot;age&quot;, &quot;books&quot;) ) + labs(y = &quot;Prob(MPL on both reading and math)&quot;, x = &quot;Child&#39;s Age in Years&quot;, title = &quot;Predicted probabilities of MPL with Age and Books&quot;) + theme_classic() These plots confirm that: There is a positive relationship between the child’s age and meeting MPL for both maths and reading. The odds of meeting MPL for both maths and reading is higher for children who bring back home study material from the school library for reading. This observation intensifies as the child’s age increases. 8.6 Interaction In the last section, we never considered potential interaction effects. In a regression model, interaction occurs when the relationship between a predictor and the outcome depends on the value or the level taken by another predictor. On the contrary, if the relationship between a predictor and the outcome remains unchanged no matter the value taken by another predictor, we cannot conclude that there is an interaction effect. In our case, there would be an interaction if the relationship between the child’s age and MPL for both maths and reading depends on the sex predictor. There would be an interaction, for instance, if the relationship between the child’s age and MPL for both maths and reading was positive for female children, and negative for male children, or vice versa. Or if the relationship between the child’s age and MPL for both maths and reading was much stronger or much weaker for female children than for male children. Let us see if there is any interaction between age and sex, and more importantly, whether or not this interaction is significant: # save model with interaction m4_inter = svyglm(mpl_both ~ age + sex + books + sex*age, family = quasibinomial, design =des, na.action = na.omit ) m4_inter |&gt; tidy() ## # A tibble: 5 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5.84 0.245 -23.8 5.81e-58 ## 2 age 0.415 0.0197 21.1 8.96e-51 ## 3 sexMale 0.666 0.278 2.40 1.76e- 2 ## 4 booksYes 0.397 0.105 3.78 2.15e- 4 ## 5 age:sexMale -0.0591 0.0234 -2.53 1.24e- 2 8.6.1 Interpretation of age, sex, and the age*sex interaction age is positively associated with MPL (\\(\\beta_{age} = 0.415\\)). For the reference group (Females), this corresponds to an odds ratio (OR) of \\(1.51\\) per one-year increase in age (OR $ = e^{0.415} = 1.514$), holding other predictors constant. The age-by-sex interaction is statistically significant (\\(\\beta_{int} = -0.059\\), p-value\\(=0.012\\)), which indicates that the age effect differs by sex. Specifically, the age slope for males equals \\(\\beta_{age}+ \\beta_{int} = 0.355\\), which corresponds to an OR of \\(1.43\\) per year (\\(e^{0.355} = 1.427\\)). Equivalently, the male-to-female odds ratio changes multiplicatively by \\(e^{\\beta_{int}} = 0.943\\) per additional year of age (\\(\\approx 5,7\\%\\) decrease per year), which implies that the relative male-female difference diminishes with increasing age. Importantly, in the presence of an interaction, the main-effect coefficient for sex (\\(\\beta_{sexMale} = 0,666\\)) represents the male-female contrast when age is \\(0\\) years old. Consequently, \\(\\beta_{sexMale}\\) should not be interpreted as an average sex effect across the observed age range. Next, let us visualize the age* sex interaction in @fig-visthree. plot_model(m4_inter, type = &quot;pred&quot;, terms = c(&quot;age&quot;, &quot;sex&quot;) ) + labs(y = &quot;Prob(MPL on both reading and math)&quot;, x = &quot;Child&#39;s Age in Years&quot;, title = &quot;&quot;) + theme_classic() Figure 8.3: Predicted probability of MPL as a function of age, stratified by sex, from a logistic regression including an age×sex interaction (bands denote 95% confidence intervals). Predictions are computed with other predictors held constant (e.g., books fixed at its reference level). Predicted probabilities from the interaction model increase with age for both sexes, which is consistent with the positive age coefficient. However, due to the negative age*sex interaction, the increase is steeper for females than for males, which leads to an age-dependent sex difference and a cross-over in the predicted curves around ~ 11 years. Confidence bands widen at the extremes of age, which reflects increased uncertainty where data is sparser. Note: When plotting predicted probabilities as a function of age and sex, other predictors (i.e., books) are held constant. 8.7 Reporting Results As we have seen before, odds ratios are useful when reporting results of binary logistic regressions. Computing these odds ratios together with the confidence intervals is not particularly difficult. However, presenting them in a table for a publication or a report can quickly become time consuming, in particular if you have many models and many independent variables. Luckily, there are two packages which saved me a lot of time and which I use almost every time I need to report results of a logistic regression. It is knitr() and kableExtra(). Here is an example with one of the models we have built previously: # load the package library(knitr) library(kableExtra) # print table of results tab = summary(m4_inter)$coefficients |&gt; as.data.frame() |&gt; tibble::rownames_to_column(&quot;term&quot;) |&gt; rename( estimate = Estimate, std.error = `Std. Error`, statistic = `t value`, p_value = `Pr(&gt;|t|)` ) |&gt; mutate( OR = exp(estimate), OR_low = exp(estimate - 1.96 * std.error), OR_high = exp(estimate + 1.96 * std.error), p_value = ifelse(p_value &lt; 0.001, &quot;&lt;0.001&quot;, sprintf(&quot;%.3f&quot;, p_value)), `OR (95% CI)` = sprintf(&quot;%.2f (%.2f, %.2f)&quot;, OR, OR_low, OR_high) ) |&gt; select(term, estimate, std.error, statistic, `OR (95% CI)`, p_value) # print the results kable( tab, booktabs = TRUE, align = c(&quot;l&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;), digits = c(NA, 3, 3, 2, NA, NA), caption = &quot;Survey-weighted logistic regression (svyglm) results&quot; ) |&gt; kable_styling( full_width = FALSE, position = &quot;left&quot;, latex_options = c(&quot;hold_position&quot;) # nice for PDF/Quarto/Rmd ) |&gt; add_header_above(c(&quot; &quot; = 1, &quot;Coefficient&quot; = 3, &quot;Odds ratio&quot; = 1, &quot; &quot; = 1)) |&gt; column_spec(1, bold = TRUE) (#tab:reporting results)(#tab:reporting results)Survey-weighted logistic regression (svyglm) results Coefficient Odds ratio term estimate std.error statistic OR (95% CI) p_value (Intercept) -5.840 0.245 -23.82 0.00 (0.00, 0.00) &lt;0.001 age 0.415 0.020 21.05 1.51 (1.46, 1.57) &lt;0.001 sexMale 0.666 0.278 2.40 1.95 (1.13, 3.36) 0.018 booksYes 0.397 0.105 3.78 1.49 (1.21, 1.83) &lt;0.001 age:sexMale -0.059 0.023 -2.53 0.94 (0.90, 0.99) 0.012 "],["a-caucus-race-and-a-long-tale.html", "Chapter 9 A caucus-race and a long tale", " Chapter 9 A caucus-race and a long tale "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
