# Logistic Regression 

Many questions in early-foundational learning research ask about the likelihood of a binary outcome - something that is either *"yes/no"* or *"0/1"*. For instance, how likely is a child to meet the minimum proficiency level in both maths and reading, given the child's age and whether the language of assessment matches with the child's home language. In this lesson, we use the PAL Network's ICAN-ICAR 2025 survey data to model binary outcomes using logistic regression.  


```{r}
#| echo: false
#| warning: false

library(tidyverse) 
library(broom)
library(survey)

# load the data 
dat = read_csv("data/2025_PAL_ICAN-ICAR_data.csv")

```

A simple linear regression can be used as a "linear probability model", but it often produces predicted values *below 0* or *above 1* and does not handle the changing variability that comes with binary outcomes. Logistic regression avoids these issues by modeling the log-odds of the outcome, which naturally maps to predicted probabilities between 0 and 1.

Because ICAN/ICAR is collected using a complex survey design (stratification, clustering, and survey weights), we fit models using the *survey* package: we first define the design with `svydesign()` and then estimate logistic regression with `svyglm()` so that standard errors and inference reflect the sampling design.

## What logistic regression models

Suppose $Y$ indicates whether a learner meets the MPL in maths, where $Y = 1$ means "meets MPL" and $Y = 0$ means "does not meet MPL". Let $X_1$ be ICAN assessment time for maths.

```{r}
#| label: fig-simplereg
#| echo: false
#| fig-cap: "Linear regression of a binary Y variable (0 = learner do not meet MPL for Maths, 1 = learner meets MPL for Maths) on a measurement X variable (child's age in years)."
#| fig-width: 6
#| fig-height: 3.5

Y       = dat$MPL_math
X       = dat$ch02


# create a survey design object 
options(
  survey.lonely.psu = "adjust",
  survey.adjust.domain.lonely = TRUE
)

des = svydesign(
                ids = ~interaction(CountryName, VillageID) + HHID, 
             strata = ~interaction(CountryName, Tier_One_Unit),    
             weights= ~HH_Weight_Provided,
             nest   = TRUE, 
             data   = dat
)


# fit the model and plot it.  
fit = svyglm(MPL_math ~ ch02,
             design = des,
             family = gaussian())

plot(dat$MPL_math ~ dat$ch02,
     xlab = "Child's Age in Years", ylab = "MPL Math", pch = ".", cex =3)
abline(coef(fit)[1], coef(fit)[2], col = "red")

```

A linear regression of a binary outcome is sometimes called a *linear probability model*. It can produce predicted values outside the $[0,1]$ range. For example in @fig-simplereg, $$\hat{Y} = -0.351 + 0.0859X_1,$$predictions exceed 1 when $X_1 > 15.8$. This is not meaningful because probabilities must lie between 0 and 1. In addition, with a binary outcome the variability changes with the mean (because $Var(Y |X) = p(1- p)$), so the constant-variance assumption behind ordinary linear regression is not appropriate here.

Logistic regression starts by modeling the probability of "success": $$
p = P(Y = 1 | X).
$$ Since $Y$ is binary, $P(Y = 0 | X) = 1-p$. The **odds** of meeting MPL are defined as $$\text{odds} = \frac{p}{1 - p}.$$ Odds range from 0 to $\infty$. For example, if $p = 0.2$, then the odds are $0.2/0.8 = 0.25$. You can say odds are 0.25 to 1,or 1 to 4.

Logistic regression assumes the **log-odds** (the logit) are linearly related with the predictors: $$
\text{log}(\frac{p}{1-p}) = \beta_0 + \beta_1X_1
$$ Solving for $p$ gives the logistic curve: $$
p = \frac{\text{exp}(\beta_0 + \beta_1X_1)}{1 + \text{exp}(\beta_0 + \beta_1X_1)}, 
$$ which always stays between 0 and 1. Here, $\beta_1$ is the change in the **log-odds** per one-unit increase in $X_1$, and $\text{exp}(\beta_1)$ is the **odds ratio** associated with a one-unit increase in $X_1$.

If the logit is a linearly related to the $X$ variables, then the probability $p$ is a non-linear, S-shaped function with respect to the $X$ variables as in @fig-logreg.

```{r}
#| label: fig-logreg
#| fig-cap: "Predicted probability as a logit function X"
#| fig-width: 6
#| fig-height: 3.5
#| echo: false

x = seq(-10, 10, length.out = 400)
y = 1 / (1 + exp(-x))   # sigmoid

plot(x, y, type = "l", lwd = 2,
     xlab = "x", ylab = "predicted P(Y = 1)")

```

Logistic regression is widely used in many fields whenever we want to predict the probability of a binary outcome (yes/no, 0/1). For example:

-   **Medicine**: to estimate the probability that a patient has (or will develop) a disease using medical history and test results.

-   **Banking/Finance**: to estimate the probability that a borrower will default, based on income, credit history, and other profile information.

-   **Politics**: to estimate the probability that a citizen will vote for a particular party or candidate.

-   **Many others**: marketing (purchase vs not), education (pass vs fail), and program evaluation (participate vs not).

In this lesson, we apply logistic regression to ICAN/ICAR survey data to model whether a learner meets the Minimum Proficiency Level (MPL).

## Simple versus multiple logistic regression

Binary logistic regression has an outcome $Y$ which has two categories (e.g., $Y =1$, $Y = 0$). We distinguish between two common forms:

-   Simple logistic regression uses one predictor: $$
    \text{log}(\frac{p}{1-p}) = \beta_0+\beta_1X_1
    $$

-   Multiple logistic regression uses two or more predictors: $$
    \text{log}(\frac{p}{1-p}) = \beta_0+\beta_1X_1+\beta_2X_2+\ldots+ \beta_kX_k
    $$

    Multiple logistic regression is often more useful because it lets us adjust for other factors. Each coefficient describes the relationship between a predictor and the outcome holding the other predictors constant.

Because the ICAN/ICAR data come from a complex survey design, we estimate both simple and multiple logistic regression models using `svmglm()` with a design object created by `svydesign()`.

## Data

We create an analysis-ready data by, (i) restricting to a specific country and learner subgroup, (ii) selecting a small set of variables needed for the models, and (iii) re-coding key variables into analysis-friendly formats (especially binary predictors and the MPL outcome).

1.  `ch02`: Child's age

2.  `ch03`: Child's sex

3.  `ch09`: Has the child brought any material to read (other than school textbooks) from the school library

4.  `MPL_both`: Whether the learner meets minimum proficiency level in both maths and reading

5.  `CountryName`: Unique country name

6.  `Tier_One_Unit`: Geographical divisions within one country (county, state, province, etc)

7.  `VillageID`: Unique village identifiers

8.  `HHID`: Unique household identifier

### Import, subset, and keep only needed variables

```{r}
#| label: load data 
#| warning: false

# load packages
library(tidyverse)
library(survey)     
library(skimr)     


# load the data 
dat = read_csv("data/2025_PAL_ICAN-ICAR_data.csv") |> 
  
       filter(
         
              CountryName == "Senegal" &                # restrict to this country
               enr_status == "Currently Enrolled"     # currently enrolled in school
          ) |> 
  
        select(
          Location, CountryName, Tier_One_Unit, VillageID, HHID, 
          HH_Weight_Provided, ch02, ch03, ch09, 
          MPL_both
        ) |>
  
       rename(
             age    = ch02, 
             sex    = ch03, 
             books  = ch09, 
         mpl_both   = MPL_both
       )

```

### Recode variables (labels + reference levels)

```{r}
#| label: convert to factors 

dat = dat |> 
       mutate(
         # Outcome (0/1 -> No/Yes)
         mpl_both = factor(mpl_both, levels = c(0,1), labels = c("No", "Yes")), 
         
         # Binary predictors (0/1 -> No/Yes)
         books = factor(books, levels = c(0,1), labels = c("No", "Yes")), 
         sex   = factor(sex, levels = c(1,2), labels = c("Female", "Male")), 
         Location  = factor(Location)
         
       )
                    

```

Always recognize the order of the levels for the dependent variable because it will have an impact on the interpretations. In R, the first level from `levels()` is always taken as the reference level.

In our case, the first level is not meeting the MPL and the second level is meeting the MPL for both maths and reading.

```{r}
#| labels: levels 
levels(dat$mpl_both)
```

This means that when we estimate the impact of the independent variable(s) it will be on the learner meeting the MPL (and not on the learner not meeting the MPL).

Here is the preview of the data and some descriptive statistics.

```{r}
dat |>
  skim()
```


### Why we need survey design information

The ICAN/ICAR data come from a complex, multi-stage sample (clustered and stratified). To get correct standard errors, we create a survey design object that tells R which variables represent strata, primary sampling units (PSUs), and sampling weights. After this step, we fit models using `svyglm()` with the design object rather than using `glm()` directly.

```{r}
#| label: create a survey design object 

options(
  survey.lonely.psu = "adjust",
  survey.adjust.domain.lonely = TRUE
)


des = svydesign(
                ids = ~interaction(CountryName, VillageID) + HHID, 
             strata = ~interaction(CountryName, Tier_One_Unit),    
             weights= ~HH_Weight_Provided,
             nest   = TRUE, 
             data   = dat
)

```

## Simple Logistic Regression

A simple logistic regression models a binary outcome using one predictor. 

We will illustrate simple logistic regression with:

1.  a continuous predictor, and
2.  a categorical predictor.

### Continuous predictor: child's age

Suppose we want to model whether a learner meets MPL for both maths and reading (`MPL_both`) using the the child's age (`age`). Here.

```{r}
#| label: cont independent variable 
#| warnings: false

m1 = svyglm(mpl_both ~ age, 
          family = quasibinomial,
          design =des, 
          na.action = na.omit
)
m1 |> 
  tidy()


```

#### Interpreting the coefficient (log-odds)

In a logistic model,

$$
\text{log}(\frac{p}{1-p}) = \beta_0+\beta_1X
$$

so $\beta_1$ is the change in log-odds for a one-unit increase in $X$ (here, one more year in child's age). If $\beta_1 > 0$: odds (and probability) of meeting MPL for both tends to increase as $X$ increases. If $\beta_1 < 0$: odds (and probability) of meeting MPL for both tend to decrease as $X$ increases.

Here, $\hat{\beta_1} \approx 0.39$, meaning that one year increase in child's age leads to $0.39$ increase in the log odds of a child meeting MPL for both maths and reading. Older children are *more likely* in odds terms to meet MPL for both maths and reading, holding the model as specified.

#### Interpreting the odds ratio

Exponentiation of the  $\hat{\beta_1}$ coefficient converts back to the odds ratio of a child meeting MPL for both $(Y = 1)$. 

```{r}
#| label: odd ratio 

exp(coef(m1)["age"])
```

We can say that an additional year in the child's age multiplies odds of meeting MPL by $~1.48$. This means that the odds of meeting MPL for both maths and reading are about $48\%$ higher per additional year in the child's age (because $1.4785 - 1 \approx 0.4785$).

Also, R performs a hypothesis test for each coefficient, that is $H_0: \beta_j = 0$ versus $H_0: \beta_j \neq 0$ for $j = [0, 1]$ via the Wald test, and prints the *p*-values in the last column. We can thus compare these *p-*values to the chosen significance level (usually $\alpha = 0.05$) to conclude whether or not each of the coefficient is significantly different from zero. The lower the *p*-value, the more evidence that the coefficient is different from zero. This is similar to the linear regression.

Here, the *p*-value $< 0.05$. This means that there is sufficient evidence that the age coefficient is statistically different from zero at the $5\%$ significance level. 


#### Predicted probabilities

A good next step is to use the model to compute the predicted probabilities for few ages. Suppose we would like to predict the probability of meeting MPL for both maths and reading for a child that is aged $7, 10, 13, 16$:

```{r}
#| label: predict 

newdat = data.frame(age = c(7, 10, 13, 16))

# get predictions on the link (logit) scale 
                pred = predict(m1, newdata = newdat)
newdat$probabilities = plogis(pred)       #convert logit --> probability

newdat

```
As the child grows in years the likelihood of them meeting the MPL for both increases substantially. We can also visualize the results of our model below:

```{r}
#| label: model plot
#| warning: false
library(sjPlot)

# plot 
plot_model(
  m1,
  type = "pred", 
  terms= "age"
) + 
  labs(y = "Probability of MPL for both maths & reading",
       x = "Child's Age in Years", 
       title = "Predicted Probabilities of MPL for both Maths & Reading using Age") +
  theme_classic()

```

The above plot shows that probability of a child meeting MPL for both reading and maths increases with the child's age. 

### Categorical predictor example: Books

Suppose we are now interested in predicting the probability of a child meeting MPL for both using whether the child has brought home any material (other than textbooks) to read from the school library (`books`). 

Recall that when the predictor was continuous, $e^{\hat{\beta_1}}$ was the multiplicative change in the odds in favor of $Y = 1$ as $X_1$ increases by one unit. With $X_1$ being `books`, a categorical variable, the only increase possible is 0 to 1 (or from 1 to 2 if `sex` is encoded as a factor). So, we can write the interpretation in terms of *Yes/No*: $e^{\hat{\beta_1}}$ is the multiplicative change of the odds in favor of $Y = 1$ as *No* becomes *Yes*, keeping in mind the order of the level for the variable `books`.

```{r}
# levels for books variable
levels(dat$books)
```

So, it is indeed the multiplicative change of the odds in favor of $Y = 1$ as **No becomes Yes.** If the level *No* came before the level *Yes* in the data, it would have been the opposite. We fit the model again:

```{r}
#| label: categorical independent variable


m2 = svyglm(mpl_both ~ books, 
          family = quasibinomial,
          design =des, 
          na.action = na.omit
)

m2 |>
  tidy()


```
#### Interpreting the coefficient (log-odds) 

As the *No* becomes *Yes*, the log-odds of meeting MPL for both changes by $0.492$. The odds (and probabilities) tend to *increase* as the child start to bring back materials to read from the school library. 


#### Interpreting the odds ratio 

Odds ratio for `books` variable are

```{r}

exp(coef(m2)["booksYes"])
```
For children that bring home study materials from the school library, their odds of meeting MPL is multiplied by a factor of $e^{0.492} = 1.635$ relative to those who do not bring back home study materials from the school library. This means that the odds of meeting MPL for both reading and maths are $(1.635 -1)*100 = 63.5\%$ higher for those children who bring home study materials from the school library than those who do not bring back home study materials.
    
    
The interpretation of the intercept $\hat{\beta_0} = -1.138$ gives the probability of a child meeting MPL when $X= 0$. In our case, this simply maps to those children who do not bring back study materials from the school library. Therefore, the probability of a a child who does not bring back home study material meeting MPL for both reading and math is:

```{r}
exp(coef(m2)[1]) / (1+exp(coef(m2)[1]))
```
The model's intercept tells us that the probability of a child that does not bring back home study materials from the school library meeting MPL for both is only $24,2\%$. 

#### Predicted probabilities

As with the continuous predictor, predictions can also be made with the `predict()` function. Suppose we want to predict the probability of a child who brings back home study material from the school library meeting MPL for both reading and math:

```{r}

# predict probability to meet MPL 
newdat = data.frame(books = c("Yes", "No"))
  pred = predict(m2,
               newdata = newdat, 
               type    = "link"
      )

newdat$probabilities = plogis(pred)

newdat
```

Based on this model, it is predicted that a child who brings back home study material from the school library has a $34,4\%$ chance of meeting MPL for both reading and math. This is higher relative to those children who do not bring back home study materials. 

We can also visualize these results using the `plot_model` function:

```{r}
# plot 

plot_model(m2, 
           type = "pred", 
           terms = "books"
           ) + 
           labs(y = "Probability of MPL for both maths & reading", 
                  title = "Predicted Probabilities of MPL for both Maths & Reading using Books"
                  ) + 
  theme_classic()
        
```

The points correspond to the predicted probabilities, and the bars correspond to their confidence intervals.


## Multiple binary logistic regression

The interpretation of the coefficients in multiple logistic regression is similar to that of simple logistic regression, except that this time it estimates the multiplicative change in the log-odds in favor of $Y = 1$ when $X$ increases by one unit, while holding other predictors constant.

For this illustration, suppose we want to predict the MPL for both reading and math using `age`. `sex`, and `books`. 

```{r}
# fit the model 
m3 = svyglm(mpl_both ~ age + sex + books, 
          family = quasibinomial,
          design =des, 
          na.action = na.omit
)

# print results
m3 |> 
  tidy()

```


Based on the *p*-values, we can conclude that, at the $5\%$ significance level, only `age` and `books` are significantly associated with MPL for both reading and math (*p-*value $< 0.05$).

Similar to simple logistic regression, it is easier to interpret these relationships through the odds ratios. But this time, we also print $95\%$ CI of the odds ratios in addition to the OR (rounded to 3 decimals) so that we can easily see which ones are significantly different from 1. If $95\%$ CI include the value of 1 in their bound, then there is no significant association between the MPL both and the predictor in question. This is another way of confirming the significance of a predictor. 


```{r}
# OR and 95% CI
round(exp(cbind(OR = coef(m3), confint(m3))), 3)
```

From the OR and their $95\%$ CI computed above, we can conclude that:

-  `age`: an additional year in the child's age multiplies odds of meeting MPL by $~1.48$.This means that the odds of meeting MPL for both maths and reading are about $48\%$ higher per additional year in the child's age, *ceteris paribus*. 

- `books`: the odds of a child that brings back home study material from the school library meeting MPL for both reading and math are $1.482$ times higher than the child who does not bring back home study material, *ceteris paribus*.

We refrain from interpreting the `sex` as it not significant at the $5\%$ significance level (also, 1 is included in its $95\%$ CI).

For illustrative purposes, suppose now we want to predict the probability that a new child meets MPL for both reading and math. Suppose that this child is from Senegal, aged 15 years old, male, and does not bring back home study material from the school library:

```{r}
# create data frame of new learner 
new_learner = data.frame( age = 15, 
                          sex = "Male", 
                          books = "No"
                          )

# predict probability to meet MPL both 
pred  = predict(m3, 
                newdata = new_learner, 
                type    = "link"
                )

# print results 
new_learner$probabilities = plogis(pred)

new_learner
```

If we *trust* our logistic regression model, the probability that this new child meets MPL for both reading and math is predicted to be $56.5\%$.

We can also visualize the effect of `age` and `books` on the predicted probability of meeting MPL in @fig-vistwo. 



```{r}
#| warning: false
#| label: fig-vistwo

plot_model(m3,
           type = "pred", 
           terms = c("age", "books") 
          
           ) + 
  labs(y = "Prob(MPL on both reading and math)",
       x = "Child's Age in Years",
       title = "Predicted probabilities of MPL with Age and Books") +
  theme_classic()
```



These plots confirm that:

-   There is a positive relationship between the child's age and meeting MPL for both maths and reading.

-   The odds of meeting MPL for both maths and reading is higher for children who bring back home study material from the school library for reading. This observation intensifies as the child's age increases. 


## Interaction

In the last section, we never considered potential interaction effects. In a regression model, interaction occurs when the relationship between a predictor and the outcome depends on the value or the level taken by another predictor. On the contrary, if the relationship between a predictor and the outcome remains unchanged no matter the value taken by another predictor, we cannot conclude that there is an interaction effect.

In our case, there would be an interaction if the relationship between the child's `age` and MPL for both maths and reading depends on the `sex` predictor. There would be an interaction, for instance, if the relationship between the child's `age` and MPL for both maths and reading was positive for female children, and negative for male children, or vice versa. Or if the relationship between the child's `age` and MPL for both maths and reading was much stronger or much weaker for female children than for male children.

Let us see if there is any interaction between `age` and `sex`, and more importantly, whether or not this interaction is significant:

```{r}
#| warning: false
#| label: interaction 

# save model with interaction 
m4_inter =  svyglm(mpl_both ~ age + sex + books + sex*age, 
          family = quasibinomial,
          design =des, 
          na.action = na.omit
)

m4_inter |>
  tidy()
```

### Interpretation of age, sex, and the age*sex interaction 
`age` is positively associated with MPL ($\beta_{age} = 0.415$). For the reference group (Females), this corresponds to an odds ratio (OR) of $1.51$ per one-year increase in `age` (OR $ = e^{0.415} = 1.514$), holding other predictors constant. 

The age-by-sex interaction is statistically significant ($\beta_{int} = -0.059$, *p*-value$=0.012$), which indicates that the `age` effect differs by `sex`. Specifically, the age slope for males equals $\beta_{age}+ \beta_{int} = 0.355$, which corresponds to an OR of $1.43$ per year ($e^{0.355} = 1.427$). Equivalently, the male-to-female odds ratio changes multiplicatively by $e^{\beta_{int}} = 0.943$ per additional year of age ($\approx 5,7\%$ decrease per year), which implies that the relative male-female difference diminishes with increasing age. 

Importantly, in the presence of an interaction, the main-effect coefficient for sex ($\beta_{sexMale} = 0,666$) represents the male-female contrast when `age` is $0$ years old. Consequently, $\beta_{sexMale}$ should not be interpreted as an average sex effect across the observed age range. Next, let us visualize the `age`* `sex` interaction in @fig-visthree. 


```{r}
#| warning: false
#| label: fig-visthree
#| fig-cap: "Predicted probability of MPL as a function of age, stratified by sex, from a logistic regression including an ageÃ—sex interaction (bands denote 95% confidence intervals). Predictions are computed with other predictors held constant (e.g., `books` fixed at its reference level)."


plot_model(m4_inter,
           type = "pred", 
           terms = c("age", "sex") 
          
           ) + 
  labs(y = "Prob(MPL on both reading and math)",
       x = "Child's Age in Years", 
       title = "") +
  theme_classic()
```
Predicted probabilities from the interaction model increase with `age` for both sexes, which is consistent with the positive `age` coefficient. However, due to the negative `age`*`sex` interaction, the increase is steeper for females than for males, which leads to an age-dependent sex difference and a cross-over in the predicted curves around ~ 11 years. Confidence bands widen at the extremes of age, which reflects increased uncertainty where data is sparser. 

Note: When plotting predicted probabilities as a function of `age` and `sex`, other predictors (i.e., `books`) are held constant.



## Reporting Results

As we have seen before, odds ratios are useful when reporting results of binary logistic regressions.

Computing these odds ratios together with the confidence intervals is not particularly difficult. However, presenting them in a table for a publication or a report can quickly become time consuming, in particular if you have many models and many independent variables.

Luckily, there are two packages which saved me a lot of time and which I use almost every time I need to report results of a logistic regression. It is `knitr()` and `kableExtra()`. Here is an example with one of the models we have built previously:

```{r}
#| label: reporting results
#| warning: false

# load the package  
library(knitr)
library(kableExtra)

# print table of results 
tab = summary(m4_inter)$coefficients |>
  as.data.frame() |>
  tibble::rownames_to_column("term") |>
  rename(
    estimate  = Estimate,
    std.error = `Std. Error`,
    statistic = `t value`,
    p_value   = `Pr(>|t|)`
  ) |>
  mutate(
    OR      = exp(estimate),
    OR_low  = exp(estimate - 1.96 * std.error),
    OR_high = exp(estimate + 1.96 * std.error),
    p_value = ifelse(p_value < 0.001, "<0.001", sprintf("%.3f", p_value)),
    `OR (95% CI)` = sprintf("%.2f (%.2f, %.2f)", OR, OR_low, OR_high)
  ) |>
  select(term, estimate, std.error, statistic, `OR (95% CI)`, p_value)


# print the results 

kable(
  tab,
  booktabs = TRUE,
  align = c("l", "r", "r", "r", "r", "r"),
  digits = c(NA, 3, 3, 2, NA, NA),
  caption = "Survey-weighted logistic regression (svyglm) results"
) |>
  kable_styling(
    full_width = FALSE,
    position = "left",
    latex_options = c("hold_position")   # nice for PDF/Quarto/Rmd
  ) |>
  add_header_above(c(" " = 1, "Coefficient" = 3, "Odds ratio" = 1, " " = 1)) |>
  column_spec(1, bold = TRUE)

```


